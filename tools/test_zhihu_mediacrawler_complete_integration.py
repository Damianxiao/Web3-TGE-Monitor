"""
MediaCrawlerçŸ¥ä¹é›†æˆæµ‹è¯•
éªŒè¯å®Œæ•´çš„æ•°æ®æµå’ŒAPIå…¼å®¹æ€§
"""
import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(override=True)

from crawler.platforms.zhihu_platform import ZhihuPlatform
from crawler.platforms.mediacrawler_zhihu_integration import create_mediacrawler_integration
from config.settings import Settings


async def test_mediacrawler_integration():
    """æµ‹è¯•MediaCrawleré›†æˆå±‚"""
    print("ğŸ§ª æµ‹è¯•MediaCrawleré›†æˆå±‚...")
    
    try:
        # åˆ›å»ºé›†æˆå®ä¾‹
        integration = await create_mediacrawler_integration()
        print("âœ… MediaCrawleré›†æˆå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        print("ğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
        results = await integration.search_content("Web3", max_results=5)
        
        if results:
            print(f"âœ… æœç´¢æˆåŠŸï¼šè·å¾— {len(results)} æ¡ç»“æœ")
            
            # æ˜¾ç¤ºç¬¬ä¸€æ¡ç»“æœçš„è¯¦ç»†ä¿¡æ¯
            first_result = results[0]
            print(f"ğŸ“ ç¬¬ä¸€æ¡ç»“æœï¼š")
            print(f"   ID: {first_result.get('id', 'N/A')}")
            print(f"   æ ‡é¢˜: {first_result.get('title', 'N/A')[:50]}...")
            print(f"   ç±»å‹: {first_result.get('content_type', 'N/A')}")
            print(f"   ä½œè€…: {first_result.get('author', {}).get('nickname', 'N/A')}")
            
            # æµ‹è¯•è¯¦æƒ…è·å–
            content_id = first_result.get('id')
            if content_id:
                print(f"ğŸ” æµ‹è¯•è¯¦æƒ…è·å–: {content_id}")
                details = await integration.get_content_details(content_id)
                if details:
                    print("âœ… è¯¦æƒ…è·å–æˆåŠŸ")
                else:
                    print("âš ï¸ è¯¦æƒ…æœªæ‰¾åˆ°ï¼ˆæ­£å¸¸ï¼Œå› ä¸ºåœ¨æœç´¢ç»“æœä¸­æŸ¥æ‰¾ï¼‰")
        else:
            print("âŒ æœç´¢è¿”å›ç©ºç»“æœ")
            return False
        
        # æ¸…ç†èµ„æº
        integration.cleanup()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ MediaCrawleré›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_zhihu_platform():
    """æµ‹è¯•é‡æ„åçš„ZhihuPlatform"""
    print("\nğŸ§ª æµ‹è¯•é‡æ„åçš„ZhihuPlatform...")
    
    try:
        # åˆ›å»ºZhihuPlatformå®ä¾‹
        platform = ZhihuPlatform()
        print("âœ… ZhihuPlatformå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        print("ğŸ” æµ‹è¯•å¹³å°æœç´¢åŠŸèƒ½...")
        raw_contents = await platform.search("Web3", max_results=3)
        
        if raw_contents:
            print(f"âœ… å¹³å°æœç´¢æˆåŠŸï¼šè·å¾— {len(raw_contents)} æ¡å†…å®¹")
            
            # éªŒè¯RawContentæ ¼å¼
            first_content = raw_contents[0]
            print(f"ğŸ“ éªŒè¯RawContentæ ¼å¼ï¼š")
            print(f"   ID: {first_content.content_id}")
            print(f"   æ ‡é¢˜: {first_content.title[:50]}...")
            print(f"   å¹³å°: {first_content.platform.value}")
            print(f"   ç±»å‹: {first_content.content_type.value}")
            print(f"   ä½œè€…: {first_content.author_name}")
            print(f"   URL: {first_content.source_url}")
            print(f"   åˆ›å»ºæ—¶é—´: {first_content.publish_time}")
            print(f"   å…ƒæ•°æ®å­—æ®µ: {list(first_content.platform_metadata.keys())}")
            
            # æµ‹è¯•è¯¦æƒ…è·å–
            print(f"ğŸ” æµ‹è¯•è¯¦æƒ…è·å–: {first_content.content_id}")
            details = await platform.get_content_details(first_content.content_id)
            if details:
                print("âœ… è¯¦æƒ…è·å–æˆåŠŸ")
                print(f"   è¯¦æƒ…æ ‡é¢˜: {details.title[:50]}...")
            else:
                print("âš ï¸ è¯¦æƒ…æœªæ‰¾åˆ°ï¼ˆæ­£å¸¸æƒ…å†µï¼‰")
        else:
            print("âŒ å¹³å°æœç´¢è¿”å›ç©ºç»“æœ")
            return False
        
        # æ¸…ç†èµ„æº
        platform.cleanup()
        print("âœ… å¹³å°èµ„æºæ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ZhihuPlatformæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_data_format_compatibility():
    """æµ‹è¯•æ•°æ®æ ¼å¼å…¼å®¹æ€§"""
    print("\nğŸ§ª æµ‹è¯•æ•°æ®æ ¼å¼å…¼å®¹æ€§...")
    
    try:
        platform = ZhihuPlatform()
        
        # è·å–æœç´¢ç»“æœ
        raw_contents = await platform.search("DeFi", max_results=2)
        
        if not raw_contents:
            print("âš ï¸ æ— æœç´¢ç»“æœï¼Œè·³è¿‡æ ¼å¼å…¼å®¹æ€§æµ‹è¯•")
            return True
        
        content = raw_contents[0]
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['content_id', 'title', 'content', 'platform', 'content_type']
        missing_fields = []
        
        for field in required_fields:
            if not hasattr(content, field) or getattr(content, field) is None:
                missing_fields.append(field)
        
        if missing_fields:
            print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
            return False
        
        # éªŒè¯å…ƒæ•°æ®ç»“æ„
        metadata = content.platform_metadata
        expected_metadata_keys = ['source_keywords', 'question_id', 'author_profile_url']
        
        for key in expected_metadata_keys:
            if key not in metadata:
                print(f"âš ï¸ å…ƒæ•°æ®ç¼ºå°‘å­—æ®µ: {key}")
        
        # éªŒè¯æ•°æ®ç±»å‹
        type_checks = [
            (content.content_id, str, 'content_id'),
            (content.title, str, 'title'),
            (content.content, str, 'content'),
            (content.platform_metadata, dict, 'platform_metadata')
        ]
        
        for value, expected_type, field_name in type_checks:
            if not isinstance(value, expected_type):
                print(f"âŒ å­—æ®µç±»å‹é”™è¯¯: {field_name} åº”ä¸º {expected_type.__name__}ï¼Œå®é™…ä¸º {type(value).__name__}")
                return False
        
        print("âœ… æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
        
        # æ¸…ç†èµ„æº
        platform.cleanup()
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†...")
    
    try:
        platform = ZhihuPlatform()
        
        # æµ‹è¯•ç©ºå…³é”®è¯æœç´¢
        print("ğŸ” æµ‹è¯•ç©ºå…³é”®è¯æœç´¢...")
        try:
            results = await platform.search("", max_results=1)
            print(f"âš ï¸ ç©ºå…³é”®è¯æœç´¢æœªæŠ¥é”™ï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
        except Exception as e:
            print(f"âœ… ç©ºå…³é”®è¯æœç´¢æ­£ç¡®æŠ›å‡ºå¼‚å¸¸: {e}")
        
        # æµ‹è¯•æ— æ•ˆå†…å®¹ID
        print("ğŸ” æµ‹è¯•æ— æ•ˆå†…å®¹ID...")
        details = await platform.get_content_details("invalid_id_12345")
        if details is None:
            print("âœ… æ— æ•ˆIDæ­£ç¡®è¿”å›None")
        else:
            print("âš ï¸ æ— æ•ˆIDæ„å¤–è¿”å›äº†ç»“æœ")
        
        # æ¸…ç†èµ„æº
        platform.cleanup()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ MediaCrawlerçŸ¥ä¹é›†æˆå®Œæ•´æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒé…ç½®
    try:
        settings = Settings()
        if not settings.zhihu_cookie:
            print("âŒ ZHIHU_COOKIEæœªé…ç½®")
            return
        if not settings.mediacrawler_path:
            print("âŒ MEDIACRAWLER_PATHæœªé…ç½®")
            return
        print("âœ… ç¯å¢ƒé…ç½®æ£€æŸ¥é€šè¿‡")
    except Exception as e:
        print(f"âŒ ç¯å¢ƒé…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        return
    
    # æ‰§è¡Œæµ‹è¯•å¥—ä»¶
    test_results = {}
    
    # æµ‹è¯•1: MediaCrawleré›†æˆå±‚
    test_results['integration'] = await test_mediacrawler_integration()
    
    # æµ‹è¯•2: ZhihuPlatform
    test_results['platform'] = await test_zhihu_platform()
    
    # æµ‹è¯•3: æ•°æ®æ ¼å¼å…¼å®¹æ€§
    test_results['compatibility'] = await test_data_format_compatibility()
    
    # æµ‹è¯•4: é”™è¯¯å¤„ç†
    test_results['error_handling'] = await test_error_handling()
    
    # æµ‹è¯•ç»“æœæ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    all_passed = all(test_results.values())
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MediaCrawleré›†æˆæˆåŠŸ")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. å®šæœŸæ›´æ–°çŸ¥ä¹Cookieä»¥ä¿æŒè®¿é—®æƒé™")
        print("   2. ç›‘æ§MediaCrawlerç‰ˆæœ¬æ›´æ–°")
        print("   3. å…³æ³¨çŸ¥ä¹åçˆ¬è™«æœºåˆ¶å˜åŒ–")
    else:
        print("\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        failed_tests = [name for name, result in test_results.items() if not result]
        print(f"   å¤±è´¥çš„æµ‹è¯•: {', '.join(failed_tests)}")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("   1. çŸ¥ä¹Cookieæ˜¯å¦æœ‰æ•ˆ")
        print("   2. MediaCrawlerè·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")


if __name__ == "__main__":
    asyncio.run(main())