# ğŸ” Web3 TGE Monitor é¡¹ç›®å®¡æŸ¥æŠ¥å‘Š

**å®¡æŸ¥æ—¥æœŸï¼š** 2025-01-14  
**å®¡æŸ¥èŒƒå›´ï¼š** å…¨é¡¹ç›®æ¶æ„ã€ä»£ç è´¨é‡ã€å®‰å…¨æ€§ã€æ€§èƒ½ã€æµ‹è¯•ã€é…ç½®  
**é¡¹ç›®ç‰ˆæœ¬ï¼š** v0.1.0

---

## ğŸ“Š å®¡æŸ¥æ‘˜è¦

| ç±»åˆ« | ä¸¥é‡é—®é¢˜ | é‡è¦é—®é¢˜ | æ”¹è¿›å»ºè®® | æ€»è®¡ |
|------|----------|----------|----------|------|
| å®‰å…¨æ€§ | 4 | 3 | 2 | 9 |
| ä»£ç è´¨é‡ | 2 | 6 | 8 | 16 |
| æ¶æ„è®¾è®¡ | 3 | 4 | 5 | 12 |
| æ€§èƒ½ä¼˜åŒ– | 1 | 3 | 4 | 8 |
| æµ‹è¯•è¦†ç›– | 2 | 2 | 3 | 7 |
| é…ç½®ç®¡ç† | 1 | 2 | 3 | 6 |
| **æ€»è®¡** | **13** | **20** | **25** | **58** |

---

## ğŸ”¥ ä¸¥é‡é—®é¢˜ (Critical Issues) - éœ€è¦ç«‹å³ä¿®å¤

### S01 - APIå¯†é’¥ç¡¬ç¼–ç é£é™©
**æ–‡ä»¶ï¼š** `.env.example`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½å¯¼è‡´APIå¯†é’¥æ³„éœ²ï¼Œé€ æˆå®‰å…¨é£é™©å’Œè´¹ç”¨æŸå¤±

**é—®é¢˜æè¿°ï¼š**
- `.env.example` æ–‡ä»¶å¯èƒ½è¢«æ„å¤–æäº¤åŒ…å«çœŸå®APIå¯†é’¥
- ç¼ºå°‘ `.env` æ–‡ä»¶çš„å®‰å…¨æ€§æ£€æŸ¥æœºåˆ¶

**ä¿®å¤å»ºè®®ï¼š**
```bash
# 1. æ·»åŠ  .env åˆ° .gitignoreï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
echo ".env" >> .gitignore

# 2. åœ¨ .env.example ä¸­æ˜ç¡®æ ‡æ³¨ç¤ºä¾‹å€¼
AI_API_KEY=your_api_key_here_DO_NOT_COMMIT_REAL_KEY

# 3. æ·»åŠ å¯åŠ¨æ—¶çš„å®‰å…¨æ£€æŸ¥
```

### S02 - SQLæ³¨å…¥é£é™©
**æ–‡ä»¶ï¼š** `src/database/crud.py:98-105, 156-163`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½é­å—SQLæ³¨å…¥æ”»å‡»ï¼Œå¯¼è‡´æ•°æ®æ³„éœ²æˆ–ç ´å

**é—®é¢˜æè¿°ï¼š**
- `search_by_keywords` å’Œ `search` æ–¹æ³•ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥æ„å»ºæŸ¥è¯¢æ¡ä»¶
- ç¼ºå°‘å‚æ•°åŒ–æŸ¥è¯¢ä¿æŠ¤

**ä¿®å¤å»ºè®®ï¼š**
```python
# é”™è¯¯åšæ³•ï¼ˆå½“å‰ï¼‰
query = query.where(TGEProject.raw_content.contains(keyword))

# æ­£ç¡®åšæ³•
from sqlalchemy import text
query = query.where(TGEProject.raw_content.contains(text(':keyword')))
# æˆ–ä½¿ç”¨ SQLAlchemy çš„å®‰å…¨æ–¹æ³•
query = query.where(TGEProject.raw_content.op('LIKE')(f'%{keyword}%'))
```

### S03 - æœªéªŒè¯ç”¨æˆ·è¾“å…¥
**æ–‡ä»¶ï¼š** `src/api/routes/projects.py:45-52, 178-185`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½å¯¼è‡´æ•°æ®æ±¡æŸ“ã€æ³¨å…¥æ”»å‡»æˆ–ç³»ç»Ÿå´©æºƒ

**é—®é¢˜æè¿°ï¼š**
- APIè·¯ç”±ç¼ºå°‘è¾“å…¥é•¿åº¦é™åˆ¶å’Œæ ¼å¼éªŒè¯
- æ²¡æœ‰å¯¹ç‰¹æ®Šå­—ç¬¦è¿›è¡Œè¿‡æ»¤æˆ–è½¬ä¹‰

**ä¿®å¤å»ºè®®ï¼š**
```python
from pydantic import Field, validator

class ProjectSearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=200, regex=r'^[a-zA-Z0-9\u4e00-\u9fa5\s\-_]+$')
    
    @validator('query')
    def validate_query(cls, v):
        # è¿‡æ»¤å±é™©å­—ç¬¦
        dangerous_chars = ['<', '>', ';', '--', '/*', '*/', 'script']
        for char in dangerous_chars:
            if char in v.lower():
                raise ValueError(f'Query contains invalid character: {char}')
        return v
```

### S04 - MediaCrawlerè·¯å¾„éå†æ¼æ´
**æ–‡ä»¶ï¼š** `src/config/settings.py:33-44`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½å¯¼è‡´è·¯å¾„éå†æ”»å‡»ï¼Œè®¿é—®ç³»ç»Ÿæ•æ„Ÿæ–‡ä»¶

**é—®é¢˜æè¿°ï¼š**
- `mediacrawler_path` éªŒè¯ä¸å……åˆ†ï¼Œå¯èƒ½è¢«æ¶æ„åˆ©ç”¨
- ç¼ºå°‘è·¯å¾„è¾¹ç•Œæ£€æŸ¥

**ä¿®å¤å»ºè®®ï¼š**
```python
@field_validator('mediacrawler_path')
@classmethod
def validate_mediacrawler_path(cls, v):
    if not v:
        return v
    
    from pathlib import Path
    import os
    
    # ç¦æ­¢è·¯å¾„éå†å­—ç¬¦
    if '..' in v or '~' in v:
        raise ValueError("Path traversal detected in mediacrawler_path")
    
    path = Path(v)
    
    # ç¡®ä¿è·¯å¾„åœ¨é¡¹ç›®ç›®å½•å†…
    if not path.is_absolute():
        project_root = Path(__file__).parent.parent.parent
        path = (project_root / v).resolve()
        
        # æ£€æŸ¥è§£æåçš„è·¯å¾„æ˜¯å¦åœ¨é¡¹ç›®æ ¹ç›®å½•å†…
        if not str(path).startswith(str(project_root.resolve())):
            raise ValueError("MediaCrawler path outside project boundary")
    
    return str(path)
```

### S05 - æ•æ„Ÿä¿¡æ¯æ—¥å¿—æ³„éœ²
**æ–‡ä»¶ï¼š** `src/crawler/platforms/weibo_platform.py:145-150`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½åœ¨æ—¥å¿—ä¸­æ³„éœ²ç”¨æˆ·å‡­æ®å’Œæ•æ„Ÿä¿¡æ¯

**é—®é¢˜æè¿°ï¼š**
- æ—¥å¿—è®°å½•å¯èƒ½åŒ…å«Cookieã€APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯
- ç¼ºå°‘æ•æ„Ÿä¿¡æ¯è¿‡æ»¤æœºåˆ¶

**ä¿®å¤å»ºè®®ï¼š**
```python
import re

def sanitize_log_data(data):
    """æ¸…ç†æ—¥å¿—ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
    if isinstance(data, dict):
        sanitized = {}
        for key, value in data.items():
            if key.lower() in ['cookie', 'authorization', 'password', 'token', 'key']:
                sanitized[key] = '[REDACTED]'
            else:
                sanitized[key] = sanitize_log_data(value)
        return sanitized
    elif isinstance(data, str):
        # æ¸…ç†å¯èƒ½çš„å‡­æ®ä¿¡æ¯
        patterns = [
            (r'cookie[=:]\s*[^;\s]+', 'cookie=[REDACTED]'),
            (r'authorization[=:]\s*[^\s]+', 'authorization=[REDACTED]'),
            (r'Bearer\s+[^\s]+', 'Bearer [REDACTED]')
        ]
        for pattern, replacement in patterns:
            data = re.sub(pattern, replacement, data, flags=re.IGNORECASE)
        return data
    return data

# åœ¨æ—¥å¿—è®°å½•å‰ä½¿ç”¨
self.logger.info("Configuration updated", config=sanitize_log_data(config))
```

### S06 - ç¼ºå°‘å¼‚æ­¥èµ„æºç®¡ç†
**æ–‡ä»¶ï¼š** `src/ai/ai_client.py:95-110`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½å¯¼è‡´èµ„æºæ³„éœ²ã€è¿æ¥æ± è€—å°½å’ŒæœåŠ¡ä¸ç¨³å®š

**é—®é¢˜æè¿°ï¼š**
- å…¨å±€AIå®¢æˆ·ç«¯å®ä¾‹æ²¡æœ‰proper cleanup
- HTTPè¿æ¥æ²¡æœ‰æ˜¾å¼å…³é—­æœºåˆ¶

**ä¿®å¤å»ºè®®ï¼š**
```python
import atexit
from contextlib import asynccontextmanager

class AIClientManager:
    def __init__(self):
        self._clients = {}
        
    async def get_client(self, config: Dict[str, Any] = None) -> AIClient:
        client_id = id(config) if config else 'default'
        if client_id not in self._clients:
            if config is None:
                from ..config.settings import ai_config
                config = ai_config
            self._clients[client_id] = AIClient(config)
        return self._clients[client_id]
    
    async def cleanup_all(self):
        for client in self._clients.values():
            await client.close()
        self._clients.clear()

# å…¨å±€ç®¡ç†å™¨
_client_manager = AIClientManager()

async def get_ai_client(config: Dict[str, Any] = None) -> AIClient:
    return await _client_manager.get_client(config)

# æ³¨å†Œæ¸…ç†å‡½æ•°
atexit.register(lambda: asyncio.create_task(_client_manager.cleanup_all()))
```

### S07 - ç¼ºå°‘æ•°æ®åº“è¿æ¥æ± ç®¡ç†
**æ–‡ä»¶ï¼š** `src/database/database.py`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½å¯¼è‡´æ•°æ®åº“è¿æ¥è€—å°½ï¼ŒæœåŠ¡å´©æºƒ

**é—®é¢˜æè¿°ï¼š**
- æ•°æ®åº“è¿æ¥é…ç½®ç¼ºå°‘è¿æ¥æ± å‚æ•°
- æ²¡æœ‰è¿æ¥è¶…æ—¶å’Œé‡è¯•æœºåˆ¶

**ä¿®å¤å»ºè®®ï¼š**
```python
# åœ¨ create_async_engine ä¸­æ·»åŠ è¿æ¥æ± é…ç½®
engine = create_async_engine(
    settings.database_url,
    pool_size=20,          # è¿æ¥æ± å¤§å°
    max_overflow=30,       # æœ€å¤§æº¢å‡ºè¿æ¥
    pool_timeout=30,       # è·å–è¿æ¥è¶…æ—¶
    pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
    pool_pre_ping=True,    # è¿æ¥é¢„æ£€æŸ¥
    echo=settings.app_debug
)
```

### S08 - æ–‡ä»¶æ“ä½œæƒé™é—®é¢˜
**æ–‡ä»¶ï¼š** `src/crawler/platforms/weibo_platform.py:115-130`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½å¯¼è‡´æƒé™æå‡æ”»å‡»æˆ–æ–‡ä»¶ç³»ç»Ÿç ´å

**é—®é¢˜æè¿°ï¼š**
- æ–‡ä»¶å†™å…¥æ“ä½œæ²¡æœ‰æƒé™æ£€æŸ¥
- ä¸´æ—¶æ–‡ä»¶åˆ›å»ºåœ¨å¯é¢„æµ‹è·¯å¾„

**ä¿®å¤å»ºè®®ï¼š**
```python
import tempfile
import os
import stat

# ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹é…ç½®æ–‡ä»¶
def update_config_safely(config_content: str, keywords: str):
    """å®‰å…¨åœ°æ›´æ–°é…ç½®æ–‡ä»¶"""
    import tempfile
    import shutil
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py') as tmp_file:
        updated_content = re.sub(
            r'KEYWORDS\s*=\s*"([^"]*)"', 
            f'KEYWORDS = "{keywords}"', 
            config_content
        )
        tmp_file.write(updated_content)
        tmp_file_path = tmp_file.name
    
    # è®¾ç½®é€‚å½“çš„æƒé™
    os.chmod(tmp_file_path, stat.S_IRUSR | stat.S_IWUSR)
    
    return tmp_file_path
```

### S09 - ç¼ºå°‘è¯·æ±‚é¢‘ç‡é™åˆ¶
**æ–‡ä»¶ï¼š** `src/api/routes/` (æ‰€æœ‰è·¯ç”±æ–‡ä»¶)  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½é­å—DDoSæ”»å‡»ï¼ŒAPIæœåŠ¡ä¸å¯ç”¨

**é—®é¢˜æè¿°ï¼š**
- APIè·¯ç”±æ²¡æœ‰é¢‘ç‡é™åˆ¶æœºåˆ¶
- ç¼ºå°‘é˜²åˆ·å’Œé˜²æ»¥ç”¨ä¿æŠ¤

**ä¿®å¤å»ºè®®ï¼š**
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# åˆ›å»ºé™æµå™¨
limiter = Limiter(key_func=get_remote_address)

# åœ¨è·¯ç”±ä¸­æ·»åŠ é™åˆ¶
@router.get("/")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡è¯·æ±‚
async def get_projects(request: Request, ...):
    pass

# æ³¨å†Œé”™è¯¯å¤„ç†
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
```

### S10 - å¤–éƒ¨å‘½ä»¤æ‰§è¡Œé£é™©
**æ–‡ä»¶ï¼š** `start.sh:45-55`  
**ä¸¥é‡ç¨‹åº¦ï¼š** ğŸ”¥ Critical  
**å½±å“ï¼š** å¯èƒ½è¢«æ³¨å…¥æ¶æ„å‘½ä»¤ï¼Œå¯¼è‡´ç³»ç»Ÿå…¥ä¾µ

**é—®é¢˜æè¿°ï¼š**
- Shellè„šæœ¬ä¸­å­˜åœ¨å‘½ä»¤æ³¨å…¥é£é™©
- ç¼ºå°‘è¾“å…¥éªŒè¯å’Œè·¯å¾„æ£€æŸ¥

**ä¿®å¤å»ºè®®ï¼š**
```bash
# æ·»åŠ ä¸¥æ ¼çš„å˜é‡æ£€æŸ¥
set -euo pipefail

# éªŒè¯è·¯å¾„å®‰å…¨
validate_path() {
    local path="$1"
    # æ£€æŸ¥è·¯å¾„éå†
    if [[ "$path" == *".."* ]]; then
        echo "âŒ æ£€æµ‹åˆ°è·¯å¾„éå†æ”»å‡»"
        exit 1
    fi
    # æ£€æŸ¥ç»å¯¹è·¯å¾„
    if [[ "$path" = /* ]]; then
        echo "âŒ ä¸å…è®¸ä½¿ç”¨ç»å¯¹è·¯å¾„"
        exit 1
    fi
}

# ä½¿ç”¨å¼•å·ä¿æŠ¤å˜é‡
python3 -c "
import os
import sys
sys.path.insert(0, '$(pwd)')  # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
from src.config.settings import settings
print(f'æ•°æ®åº“é…ç½®: {settings.mysql_host}:{settings.mysql_port}/{settings.mysql_db}')
"
```

---

## âš ï¸ é‡è¦é—®é¢˜ (Important Issues) - éœ€è¦ä¼˜å…ˆå¤„ç†

### I01 - æ•°æ®æ¨¡å‹ä¸ä¸€è‡´
**æ–‡ä»¶ï¼š** `src/database/models.py:45-65` vs `src/api/routes/projects.py:85-95`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** æ•°æ®æ¨¡å‹ä¸APIå“åº”ä¸åŒ¹é…ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±æˆ–é”™è¯¯

**é—®é¢˜æè¿°ï¼š**
- æ•°æ®åº“æ¨¡å‹ä¸­çš„å­—æ®µä¸APIå“åº”æ¨¡å‹ä¸ä¸€è‡´
- `TGEProject` æ¨¡å‹ç¼ºå°‘ä¸€äº›APIä¸­ä½¿ç”¨çš„å­—æ®µ

**ä¿®å¤å»ºè®®ï¼š**
```python
# åœ¨ TGEProject æ¨¡å‹ä¸­æ·»åŠ ç¼ºå¤±å­—æ®µ
investment_rating = Column(String(20), nullable=True, comment="æŠ•èµ„è¯„çº§")
investment_recommendation = Column(String(50), nullable=True, comment="æŠ•èµ„å»ºè®®")
investment_reason = Column(Text, nullable=True, comment="æŠ•èµ„ç†ç”±")
key_advantages = Column(Text, nullable=True, comment="ä¸»è¦ä¼˜åŠ¿ï¼ˆé€—å·åˆ†éš”ï¼‰")
key_risks = Column(Text, nullable=True, comment="ä¸»è¦é£é™©ï¼ˆé€—å·åˆ†éš”ï¼‰")
potential_score = Column(Float, nullable=True, comment="æ½œåŠ›è¯„åˆ†")
overall_score = Column(Float, nullable=True, comment="ç»¼åˆè¯„åˆ†")
sentiment_score = Column(Float, nullable=True, comment="æƒ…æ„Ÿè¯„åˆ†")
sentiment_label = Column(String(20), nullable=True, comment="æƒ…æ„Ÿæ ‡ç­¾")
market_sentiment = Column(String(20), nullable=True, comment="å¸‚åœºæƒ…ç»ª")
analysis_confidence = Column(Float, nullable=True, comment="åˆ†æç½®ä¿¡åº¦")
```

### I02 - é”™è¯¯å¤„ç†ä¸ä¸€è‡´
**æ–‡ä»¶ï¼š** `src/crawler/platforms/weibo_platform.py:200-250`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** é”™è¯¯ä¿¡æ¯æ··ä¹±ï¼Œéš¾ä»¥è°ƒè¯•å’Œç›‘æ§

**é—®é¢˜æè¿°ï¼š**
- ä¸åŒæ¨¡å—ä½¿ç”¨ä¸åŒçš„é”™è¯¯å¤„ç†æ¨¡å¼
- å¼‚å¸¸ä¿¡æ¯æ ¼å¼ä¸ç»Ÿä¸€

**ä¿®å¤å»ºè®®ï¼š**
```python
# åˆ›å»ºç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†å™¨
class TGEMonitorException(Exception):
    """é¡¹ç›®åŸºç¡€å¼‚å¸¸ç±»"""
    def __init__(self, message: str, error_code: str = None, details: dict = None):
        self.message = message
        self.error_code = error_code or "UNKNOWN_ERROR"
        self.details = details or {}
        super().__init__(self.message)

class CrawlerException(TGEMonitorException):
    """çˆ¬è™«ç›¸å…³å¼‚å¸¸"""
    pass

class AIProcessingException(TGEMonitorException):
    """AIå¤„ç†å¼‚å¸¸"""
    pass

# ç»Ÿä¸€é”™è¯¯å¤„ç†è£…é¥°å™¨
def handle_errors(error_type: str = "GENERAL"):
    def decorator(func):
        async def wrapper(*args, **kwargs):
            try:
                return await func(*args, **kwargs)
            except TGEMonitorException:
                raise
            except Exception as e:
                logger.error(f"{error_type} error", error=str(e), function=func.__name__)
                raise TGEMonitorException(
                    message=f"{error_type} operation failed",
                    error_code=f"{error_type}_ERROR",
                    details={"original_error": str(e)}
                )
        return wrapper
    return decorator
```

### I03 - ç¼ºå°‘APIç‰ˆæœ¬æ§åˆ¶
**æ–‡ä»¶ï¼š** `src/api/routes/` (æ‰€æœ‰è·¯ç”±æ–‡ä»¶)  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** APIå‡çº§æ—¶å¯èƒ½ç ´åç°æœ‰å®¢æˆ·ç«¯

**é—®é¢˜æè¿°ï¼š**
- APIè·¯ç”±æ²¡æœ‰ç‰ˆæœ¬å‰ç¼€
- ç¼ºå°‘ç‰ˆæœ¬å…¼å®¹æ€§æœºåˆ¶

**ä¿®å¤å»ºè®®ï¼š**
```python
# åœ¨ä¸»åº”ç”¨ä¸­æ·»åŠ ç‰ˆæœ¬è·¯ç”±
from fastapi import APIRouter

# åˆ›å»ºç‰ˆæœ¬åŒ–çš„è·¯ç”±å™¨
v1_router = APIRouter(prefix="/api/v1", tags=["v1"])
v2_router = APIRouter(prefix="/api/v2", tags=["v2"])

# æ³¨å†Œè·¯ç”±
v1_router.include_router(projects.router, prefix="/projects")
v1_router.include_router(crawler.router, prefix="/crawler")

app.include_router(v1_router)

# æ·»åŠ APIç‰ˆæœ¬ä¿¡æ¯ç«¯ç‚¹
@app.get("/api/version")
async def get_api_version():
    return {
        "current_version": "v1",
        "supported_versions": ["v1"],
        "deprecated_versions": [],
        "api_docs": {
            "v1": "/docs"
        }
    }
```

### I04 - æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½é—®é¢˜
**æ–‡ä»¶ï¼š** `src/database/crud.py:250-280`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** æŸ¥è¯¢å“åº”æ…¢ï¼Œç”¨æˆ·ä½“éªŒå·®

**é—®é¢˜æè¿°ï¼š**
- å¤æ‚æŸ¥è¯¢ç¼ºå°‘ç´¢å¼•ä¼˜åŒ–
- æ²¡æœ‰æŸ¥è¯¢ç»“æœç¼“å­˜

**ä¿®å¤å»ºè®®ï¼š**
```python
# æ·»åŠ å¤åˆç´¢å¼•
__table_args__ = (
    Index('idx_created_at', 'created_at'),
    Index('idx_project_name', 'project_name'),
    Index('idx_source_platform', 'source_platform'),
    Index('idx_sentiment', 'sentiment'),
    Index('idx_is_processed', 'is_processed'),
    Index('idx_tge_date', 'tge_date'),
    # æ·»åŠ å¤åˆç´¢å¼•
    Index('idx_platform_processed', 'source_platform', 'is_processed'),
    Index('idx_category_risk', 'project_category', 'risk_level'),
    Index('idx_date_platform', 'created_at', 'source_platform'),
)

# æ·»åŠ æŸ¥è¯¢ç¼“å­˜
from functools import lru_cache
import redis

redis_client = redis.Redis(host=settings.redis_host, port=settings.redis_port, db=settings.redis_db)

@lru_cache(maxsize=100)
async def get_cached_project_stats():
    """ç¼“å­˜é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯"""
    cache_key = "project_stats"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    stats = await TGEProjectCRUD.get_statistics(session)
    redis_client.setex(cache_key, 300, json.dumps(stats))  # 5åˆ†é’Ÿç¼“å­˜
    return stats
```

### I05 - é…ç½®éªŒè¯ä¸å®Œæ•´
**æ–‡ä»¶ï¼š** `src/config/settings.py:65-85`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** é…ç½®é”™è¯¯å¯èƒ½å¯¼è‡´è¿è¡Œæ—¶æ•…éšœ

**é—®é¢˜æè¿°ï¼š**
- æ•°æ®åº“è¿æ¥å‚æ•°ç¼ºå°‘éªŒè¯
- AI APIé…ç½®æ²¡æœ‰è¿é€šæ€§æ£€æŸ¥

**ä¿®å¤å»ºè®®ï¼š**
```python
from pydantic import validator, Field
import httpx

class Settings(BaseSettings):
    # æ·»åŠ å­—æ®µéªŒè¯
    mysql_port: int = Field(3306, ge=1, le=65535)
    ai_max_tokens: int = Field(1688, ge=1, le=8192)
    ai_temperature: float = Field(0.5, ge=0.0, le=2.0)
    
    @validator('database_url')
    def validate_database_url(cls, v):
        """éªŒè¯æ•°æ®åº“URLæ ¼å¼"""
        if not v.startswith(('mysql+aiomysql://', 'mysql+pymysql://')):
            raise ValueError('Database URL must use mysql+aiomysql:// or mysql+pymysql://')
        return v
    
    @validator('ai_api_base_url')
    def validate_ai_api_url(cls, v):
        """éªŒè¯AI API URL"""
        if not v.startswith(('http://', 'https://')):
            v = f'https://{v}'
        return v
    
    def validate_connections(self):
        """éªŒè¯å¤–éƒ¨æœåŠ¡è¿æ¥"""
        async def check_ai_api():
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(f"https://{self.ai_api_base_url}/v1/models")
                    return response.status_code == 200
            except:
                return False
        
        return asyncio.run(check_ai_api())
```

### I06 - æµ‹è¯•è¦†ç›–ç‡ä¸è¶³
**æ–‡ä»¶ï¼š** `tests/` (æ•´ä¸ªæµ‹è¯•ç›®å½•)  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** ä»£ç è´¨é‡æ— æ³•ä¿è¯ï¼Œå®¹æ˜“å¼•å…¥bug

**é—®é¢˜æè¿°ï¼š**
- ç¼ºå°‘APIè·¯ç”±çš„é›†æˆæµ‹è¯•
- æ²¡æœ‰çˆ¬è™«å¹³å°çš„æ¨¡æ‹Ÿæµ‹è¯•
- AIå¤„ç†æ¨¡å—ç¼ºå°‘æµ‹è¯•

**ä¿®å¤å»ºè®®ï¼š**
```python
# æ·»åŠ APIé›†æˆæµ‹è¯•
@pytest.mark.asyncio
async def test_api_projects_integration(test_client, test_db):
    """æµ‹è¯•é¡¹ç›®APIå®Œæ•´æµç¨‹"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_project = await TGEProjectCRUD.create(test_db, test_project_data)
    
    # æµ‹è¯•è·å–é¡¹ç›®åˆ—è¡¨
    response = await test_client.get("/api/v1/projects/")
    assert response.status_code == 200
    data = response.json()
    assert data["success"] is True
    assert len(data["data"]["items"]) > 0
    
    # æµ‹è¯•é¡¹ç›®è¯¦æƒ…
    response = await test_client.get(f"/api/v1/projects/{test_project.id}")
    assert response.status_code == 200
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    response = await test_client.get("/api/v1/projects/search?query=test")
    assert response.status_code == 200

# æ·»åŠ çˆ¬è™«æ¨¡æ‹Ÿæµ‹è¯•
@pytest.mark.asyncio
async def test_weibo_platform_mock(monkeypatch):
    """æµ‹è¯•å¾®åšå¹³å°ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    mock_data = [{"mblog": {"id": "123", "text": "test content"}}]
    
    async def mock_search(*args, **kwargs):
        return mock_data
    
    monkeypatch.setattr("media_platform.weibo.client.search", mock_search)
    
    platform = WeiboPlatform()
    results = await platform.crawl(["test"])
    assert len(results) > 0
```

### I07 - æ—¥å¿—é…ç½®ä¸å®Œæ•´
**æ–‡ä»¶ï¼š** `src/utils/logger.py:25-40`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** ç”Ÿäº§ç¯å¢ƒæ—¥å¿—åˆ†æå›°éš¾ï¼Œé—®é¢˜æ’æŸ¥æ•ˆç‡ä½

**é—®é¢˜æè¿°ï¼š**
- ç¼ºå°‘æ—¥å¿—è½®è½¬é…ç½®
- æ²¡æœ‰ä¸åŒçº§åˆ«çš„æ—¥å¿—åˆ†ç¦»
- ç¼ºå°‘ç»“æ„åŒ–æ—¥å¿—å­—æ®µ

**ä¿®å¤å»ºè®®ï¼š**
```python
import logging.handlers
from pathlib import Path

def configure_logging() -> None:
    """é…ç½®å®Œæ•´çš„æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path("data/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—è½®è½¬
    file_handler = logging.handlers.RotatingFileHandler(
        log_dir / "app.log",
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    
    error_handler = logging.handlers.RotatingFileHandler(
        log_dir / "error.log",
        maxBytes=10*1024*1024,
        backupCount=5
    )
    error_handler.setLevel(logging.ERROR)
    
    # ç»“æ„åŒ–æ—¥å¿—å¤„ç†å™¨
    processors = [
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="ISO"),
        structlog.processors.add_caller_info,
        structlog.processors.format_exc_info,
        # æ·»åŠ è¯·æ±‚IDï¼ˆå¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­ï¼‰
        add_request_id,
    ]
    
    structlog.configure(
        processors=processors,
        wrapper_class=structlog.stdlib.BoundLogger,
        logger_factory=structlog.stdlib.LoggerFactory(),
        context_class=dict,
        cache_logger_on_first_use=True,
    )

def add_request_id(logger, method_name, event_dict):
    """æ·»åŠ è¯·æ±‚IDåˆ°æ—¥å¿—ä¸Šä¸‹æ–‡"""
    # ä»ä¸Šä¸‹æ–‡å˜é‡è·å–è¯·æ±‚ID
    request_id = getattr(contextvars, 'request_id', None)
    if request_id:
        event_dict['request_id'] = request_id.get()
    return event_dict
```

### I08 - MediaCrawleré›†æˆè„†å¼±
**æ–‡ä»¶ï¼š** `src/crawler/platforms/weibo_platform.py:75-95`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** MediaCrawleræ›´æ–°æ—¶å¯èƒ½å¯¼è‡´é›†æˆå¤±æ•ˆ

**é—®é¢˜æè¿°ï¼š**
- ç›´æ¥ä¾èµ–MediaCrawlerå†…éƒ¨å®ç°
- ç¼ºå°‘ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
- æ²¡æœ‰é™çº§æ–¹æ¡ˆ

**ä¿®å¤å»ºè®®ï¼š**
```python
class MediaCrawlerAdapter:
    """MediaCrawleré€‚é…å™¨ï¼Œæä¾›ç‰ˆæœ¬å…¼å®¹æ€§"""
    
    SUPPORTED_VERSIONS = ["1.0.0", "1.1.0"]
    
    def __init__(self, mediacrawler_path: str):
        self.path = mediacrawler_path
        self.version = self._detect_version()
        self._validate_compatibility()
    
    def _detect_version(self) -> str:
        """æ£€æµ‹MediaCrawlerç‰ˆæœ¬"""
        try:
            version_file = Path(self.path) / "version.txt"
            if version_file.exists():
                return version_file.read_text().strip()
            # ä»setup.pyæˆ–pyproject.tomlæ£€æµ‹ç‰ˆæœ¬
            return "unknown"
        except Exception:
            return "unknown"
    
    def _validate_compatibility(self):
        """éªŒè¯ç‰ˆæœ¬å…¼å®¹æ€§"""
        if self.version not in self.SUPPORTED_VERSIONS and self.version != "unknown":
            logger.warning(
                "Unsupported MediaCrawler version",
                detected_version=self.version,
                supported_versions=self.SUPPORTED_VERSIONS
            )
    
    def get_compatible_client(self, platform: str):
        """è·å–å…¼å®¹çš„å®¢æˆ·ç«¯"""
        if self.version == "1.0.0":
            return self._get_v1_client(platform)
        elif self.version == "1.1.0":
            return self._get_v1_1_client(platform)
        else:
            # å°è¯•æœ€æ–°ç‰ˆæœ¬æ¥å£
            return self._get_latest_client(platform)
```

### I09 - æ•°æ®å»é‡é€»è¾‘ä¸å®Œå–„
**æ–‡ä»¶ï¼š** `src/utils/deduplication.py` (å¦‚æœå­˜åœ¨) / `src/database/crud.py:25-35`  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** å¯èƒ½å­˜å‚¨é‡å¤æ•°æ®ï¼Œæµªè´¹å­˜å‚¨ç©ºé—´å’Œå¤„ç†èµ„æº

**é—®é¢˜æè¿°ï¼š**
- ä»…åŸºäº`content_hash`å»é‡ï¼Œå¯èƒ½ä¸å¤Ÿç²¾ç¡®
- æ²¡æœ‰å¤„ç†å†…å®¹å¾®å°å˜åŒ–çš„æƒ…å†µ

**ä¿®å¤å»ºè®®ï¼š**
```python
import hashlib
from difflib import SequenceMatcher

class ContentDeduplicator:
    """å†…å®¹å»é‡å™¨"""
    
    @staticmethod
    def generate_content_hash(content: str, normalize: bool = True) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        if normalize:
            # æ ‡å‡†åŒ–å†…å®¹ï¼šç§»é™¤ç©ºç™½ã€è½¬æ¢å¤§å°å†™
            content = re.sub(r'\s+', ' ', content.strip().lower())
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            content = re.sub(r'[^\w\s\u4e00-\u9fa5]', '', content)
        
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    @staticmethod
    def calculate_similarity(content1: str, content2: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, content1, content2).ratio()
    
    @staticmethod
    async def check_duplicate(session: AsyncSession, content: str, similarity_threshold: float = 0.9) -> Optional[TGEProject]:
        """æ£€æŸ¥é‡å¤å†…å®¹ï¼ˆåŒ…æ‹¬ç›¸ä¼¼å†…å®¹ï¼‰"""
        content_hash = ContentDeduplicator.generate_content_hash(content)
        
        # é¦–å…ˆæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        exact_match = await TGEProjectCRUD.get_by_content_hash(session, content_hash)
        if exact_match:
            return exact_match
        
        # æ£€æŸ¥ç›¸ä¼¼å†…å®¹ï¼ˆä»…æ£€æŸ¥æœ€è¿‘çš„é¡¹ç›®ä»¥æé«˜æ€§èƒ½ï¼‰
        recent_projects = await TGEProjectCRUD.get_latest(session, limit=100)
        
        for project in recent_projects:
            similarity = ContentDeduplicator.calculate_similarity(content, project.raw_content)
            if similarity > similarity_threshold:
                logger.info("Similar content detected", 
                          similarity=similarity,
                          existing_project_id=project.id)
                return project
        
        return None
```

### I10 - APIå“åº”æ ¼å¼ä¸ç»Ÿä¸€
**æ–‡ä»¶ï¼š** `src/api/models/responses.py` vs å„è·¯ç”±æ–‡ä»¶  
**ä¸¥é‡ç¨‹åº¦ï¼š** âš ï¸ Important  
**å½±å“ï¼š** å®¢æˆ·ç«¯é›†æˆå›°éš¾ï¼ŒAPIä½¿ç”¨ä½“éªŒå·®

**é—®é¢˜æè¿°ï¼š**
- æˆåŠŸå’Œé”™è¯¯å“åº”æ ¼å¼ä¸ä¸€è‡´
- ç¼ºå°‘ç»Ÿä¸€çš„å“åº”åŒ…è£…å™¨

**ä¿®å¤å»ºè®®ï¼š**
```python
from typing import Generic, TypeVar, Optional, Any
from pydantic import BaseModel

T = TypeVar('T')

class StandardResponse(BaseModel, Generic[T]):
    """æ ‡å‡†APIå“åº”æ ¼å¼"""
    success: bool
    message: str
    data: Optional[T] = None
    error_code: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    request_id: Optional[str] = None

class ErrorDetail(BaseModel):
    """é”™è¯¯è¯¦æƒ…"""
    field: Optional[str] = None
    code: str
    message: str

class ErrorResponse(BaseModel):
    """é”™è¯¯å“åº”"""
    success: bool = False
    message: str
    error_code: str
    errors: Optional[List[ErrorDetail]] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    request_id: Optional[str] = None

# ç»Ÿä¸€å“åº”æ„é€ å‡½æ•°
def success_response(data: T = None, message: str = "æ“ä½œæˆåŠŸ") -> StandardResponse[T]:
    return StandardResponse(
        success=True,
        message=message,
        data=data
    )

def error_response(message: str, error_code: str = "OPERATION_FAILED", errors: List[ErrorDetail] = None) -> ErrorResponse:
    return ErrorResponse(
        message=message,
        error_code=error_code,
        errors=errors or []
    )
```

---

## ğŸ’¡ æ”¹è¿›å»ºè®® (Improvement Suggestions) - å»ºè®®ä¼˜åŒ–

### R01 - æ·»åŠ ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†
**å»ºè®®æ·»åŠ ï¼š** PrometheusæŒ‡æ ‡æ”¶é›†å’ŒGrafanaä»ªè¡¨æ¿

**å®ç°å»ºè®®ï¼š**
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('api_request_duration_seconds', 'Request duration')
CRAWLER_ITEMS = Counter('crawler_items_total', 'Crawled items', ['platform', 'status'])
AI_TOKENS = Counter('ai_tokens_used_total', 'AI tokens used', ['model'])

# ä¸­é—´ä»¶
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=str(request.url.path),
        status=response.status_code
    ).inc()
    
    REQUEST_DURATION.observe(duration)
    return response

# æŒ‡æ ‡ç«¯ç‚¹
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

### R02 - å®ç°é…ç½®çƒ­é‡è½½
**ç›®çš„ï¼š** æ”¯æŒè¿è¡Œæ—¶é…ç½®æ›´æ–°ï¼Œæ— éœ€é‡å¯æœåŠ¡

**å®ç°å»ºè®®ï¼š**
```python
import asyncio
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigWatcher(FileSystemEventHandler):
    def __init__(self, config_manager):
        self.config_manager = config_manager
    
    def on_modified(self, event):
        if event.src_path.endswith('.env'):
            logger.info("Config file changed, reloading...")
            asyncio.create_task(self.config_manager.reload())

class ConfigManager:
    def __init__(self):
        self.settings = Settings()
        self._watchers = []
    
    async def reload(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        old_settings = self.settings
        self.settings = Settings()
        
        # é€šçŸ¥æœåŠ¡ç»„ä»¶é…ç½®å·²æ›´æ–°
        await self._notify_config_change(old_settings, self.settings)
    
    async def _notify_config_change(self, old_config, new_config):
        """é€šçŸ¥é…ç½®å˜æ›´"""
        if old_config.ai_api_key != new_config.ai_api_key:
            # é‡æ–°åˆå§‹åŒ–AIå®¢æˆ·ç«¯
            await reinitialize_ai_client()
```

### R03 - æ·»åŠ APIæ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ
**ç›®çš„ï¼š** å®Œå–„APIæ–‡æ¡£ï¼Œæé«˜å¼€å‘ä½“éªŒ

**å®ç°å»ºè®®ï¼š**
```python
from fastapi.openapi.utils import get_openapi

def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    
    openapi_schema = get_openapi(
        title="Web3 TGE Monitor API",
        version="1.0.0",
        description="""
        Web3 TGEç›‘æ§å’ŒAIåˆ†æAPIç³»ç»Ÿ
        
        ## åŠŸèƒ½ç‰¹æ€§
        - ğŸ” å¤šå¹³å°å†…å®¹çˆ¬å–ï¼ˆå¾®åšã€å°çº¢ä¹¦ã€çŸ¥ä¹ï¼‰
        - ğŸ¤– AIé©±åŠ¨çš„å†…å®¹åˆ†æå’ŒæŠ•èµ„å»ºè®®
        - ğŸ“Š å®æ—¶æ•°æ®ç»Ÿè®¡å’Œè¶‹åŠ¿åˆ†æ
        - ğŸ” ä¼ä¸šçº§å®‰å…¨å’Œæ€§èƒ½ä¼˜åŒ–
        
        ## è®¤è¯æ–¹å¼
        ç›®å‰ä¸ºå¼€æ”¾APIï¼Œæœªæ¥å°†æ”¯æŒAPI Keyè®¤è¯
        
        ## é™æµè¯´æ˜
        - æ™®é€šæ¥å£ï¼š100è¯·æ±‚/åˆ†é’Ÿ
        - æœç´¢æ¥å£ï¼š20è¯·æ±‚/åˆ†é’Ÿ
        - çˆ¬å–æ¥å£ï¼š5è¯·æ±‚/åˆ†é’Ÿ
        """,
        routes=app.routes,
    )
    
    # æ·»åŠ è‡ªå®šä¹‰ä¿¡æ¯
    openapi_schema["info"]["contact"] = {
        "name": "å¼€å‘å›¢é˜Ÿ",
        "email": "dev@example.com",
        "url": "https://github.com/your-repo"
    }
    
    openapi_schema["info"]["license"] = {
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT"
    }
    
    app.openapi_schema = openapi_schema
    return app.openapi_schema

app.openapi = custom_openapi
```

### R04 - å®ç°æ•°æ®åº“è¿ç§»ç®¡ç†
**ç›®çš„ï¼š** å®‰å…¨åœ°ç®¡ç†æ•°æ®åº“ç»“æ„å˜æ›´

**å®ç°å»ºè®®ï¼š**
```python
# ä½¿ç”¨Alembicè¿›è¡Œæ•°æ®åº“è¿ç§»
# alembic.ini
[alembic]
script_location = migrations
sqlalchemy.url = driver://user:pass@localhost/dbname

# migrations/env.py
from src.database.models import Base
target_metadata = Base.metadata

# åˆ›å»ºè¿ç§»è„šæœ¬
alembic revision --autogenerate -m "Add new fields to TGEProject"

# æ‰§è¡Œè¿ç§»
alembic upgrade head

# åœ¨ä»£ç ä¸­æ·»åŠ è¿ç§»æ£€æŸ¥
async def check_database_version():
    """æ£€æŸ¥æ•°æ®åº“ç‰ˆæœ¬"""
    from alembic.config import Config
    from alembic import command
    
    alembic_cfg = Config("alembic.ini")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»
    # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨è¿ç§»é€»è¾‘ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
```

### R05 - å®ç°ç¼“å­˜ç­–ç•¥
**ç›®çš„ï¼š** æé«˜APIå“åº”é€Ÿåº¦ï¼Œå‡å°‘æ•°æ®åº“å‹åŠ›

**å®ç°å»ºè®®ï¼š**
```python
from functools import wraps
import pickle
import redis

redis_client = redis.Redis(host=settings.redis_host, port=settings.redis_port, db=settings.redis_db)

def cached(expiration: int = 300, key_prefix: str = "cache"):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return pickle.loads(cached_result)
            
            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            redis_client.setex(cache_key, expiration, pickle.dumps(result))
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cached(expiration=600, key_prefix="projects")
async def get_project_stats():
    """è·å–é¡¹ç›®ç»Ÿè®¡ï¼ˆç¼“å­˜10åˆ†é’Ÿï¼‰"""
    return await TGEProjectCRUD.get_statistics(session)
```

### R06 - æ·»åŠ å¥åº·æ£€æŸ¥å’Œå°±ç»ªæ£€æŸ¥
**ç›®çš„ï¼š** æ”¯æŒå®¹å™¨åŒ–éƒ¨ç½²å’ŒæœåŠ¡ç›‘æ§

**å®ç°å»ºè®®ï¼š**
```python
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "timestamp": datetime.utcnow()}

@app.get("/ready")
async def readiness_check():
    """å°±ç»ªæ£€æŸ¥ç«¯ç‚¹"""
    checks = {}
    overall_status = "ready"
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    try:
        await check_database_connection()
        checks["database"] = "ok"
    except Exception as e:
        checks["database"] = f"error: {str(e)}"
        overall_status = "not_ready"
    
    # æ£€æŸ¥Redisè¿æ¥
    try:
        redis_client.ping()
        checks["redis"] = "ok"
    except Exception as e:
        checks["redis"] = f"error: {str(e)}"
        overall_status = "not_ready"
    
    # æ£€æŸ¥AI API
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"https://{settings.ai_api_base_url}/v1/models", timeout=5)
            checks["ai_api"] = "ok" if response.status_code == 200 else f"status: {response.status_code}"
    except Exception as e:
        checks["ai_api"] = f"error: {str(e)}"
        overall_status = "not_ready"
    
    status_code = 200 if overall_status == "ready" else 503
    return JSONResponse(
        status_code=status_code,
        content={
            "status": overall_status,
            "checks": checks,
            "timestamp": datetime.utcnow().isoformat()
        }
    )
```

### R07 - å®ç°å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
**ç›®çš„ï¼š** å¤„ç†è€—æ—¶çš„çˆ¬å–å’ŒAIåˆ†æä»»åŠ¡

**å®ç°å»ºè®®ï¼š**
```python
from celery import Celery
from celery.result import AsyncResult

# åˆ›å»ºCeleryåº”ç”¨
celery_app = Celery(
    "tge_monitor",
    broker=f"redis://{settings.redis_host}:{settings.redis_port}/{settings.redis_db}",
    backend=f"redis://{settings.redis_host}:{settings.redis_port}/{settings.redis_db}"
)

@celery_app.task
def crawl_platform_task(platform: str, keywords: list, max_count: int):
    """å¼‚æ­¥çˆ¬å–ä»»åŠ¡"""
    from src.crawler.platform_factory import PlatformFactory
    
    platform_instance = PlatformFactory.create_platform(platform)
    results = asyncio.run(platform_instance.crawl(keywords, max_count))
    
    return {
        "platform": platform,
        "item_count": len(results),
        "status": "completed"
    }

@celery_app.task
def ai_analysis_task(project_id: int):
    """å¼‚æ­¥AIåˆ†æä»»åŠ¡"""
    from src.ai.processing_manager import AIProcessingManager
    
    processor = AIProcessingManager()
    result = asyncio.run(processor.process_project(project_id))
    
    return result

# APIç«¯ç‚¹
@app.post("/api/v1/tasks/crawl")
async def start_crawl_task(request: CrawlRequest):
    """å¯åŠ¨çˆ¬å–ä»»åŠ¡"""
    task = crawl_platform_task.delay(
        platform=request.platform,
        keywords=request.keywords,
        max_count=request.max_count
    )
    
    return {"task_id": task.id, "status": "started"}

@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    result = AsyncResult(task_id, app=celery_app)
    
    return {
        "task_id": task_id,
        "status": result.status,
        "result": result.result if result.ready() else None
    }
```

### R08 - æ·»åŠ æ•°æ®å¯¼å‡ºåŠŸèƒ½
**ç›®çš„ï¼š** æ”¯æŒæ•°æ®å¤‡ä»½å’Œåˆ†æ

**å®ç°å»ºè®®ï¼š**
```python
import pandas as pd
from io import StringIO, BytesIO

@app.get("/api/v1/export/projects")
async def export_projects(
    format: str = Query("csv", enum=["csv", "excel", "json"]),
    filters: dict = None,
    db: AsyncSession = Depends(get_db)
):
    """å¯¼å‡ºé¡¹ç›®æ•°æ®"""
    
    # è·å–æ•°æ®
    projects, _ = await TGEProjectCRUD.get_paginated(
        session=db,
        page=1,
        size=10000,  # å¤§æ‰¹é‡å¯¼å‡º
        filters=filters
    )
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    data = [
        {
            "id": p.id,
            "project_name": p.project_name,
            "token_symbol": p.token_symbol,
            "project_category": p.project_category,
            "risk_level": p.risk_level,
            "investment_rating": p.investment_rating,
            "created_at": p.created_at.isoformat(),
            "tge_date": p.tge_date
        }
        for p in projects
    ]
    
    if format == "csv":
        df = pd.DataFrame(data)
        output = StringIO()
        df.to_csv(output, index=False)
        
        return Response(
            content=output.getvalue(),
            media_type="text/csv",
            headers={"Content-Disposition": "attachment; filename=projects.csv"}
        )
    
    elif format == "excel":
        df = pd.DataFrame(data)
        output = BytesIO()
        df.to_excel(output, index=False)
        
        return Response(
            content=output.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": "attachment; filename=projects.xlsx"}
        )
    
    else:  # json
        return {"data": data, "count": len(data)}
```

### R09 - å®ç°å†…å®¹åˆ†ç±»å’Œæ ‡ç­¾
**ç›®çš„ï¼š** æé«˜å†…å®¹ç»„ç»‡å’Œæ£€ç´¢èƒ½åŠ›

**å®ç°å»ºè®®ï¼š**
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import jieba

class ContentClassifier:
    """å†…å®¹åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.classifier = KMeans(n_clusters=10)
        self.categories = {
            0: "DeFié¡¹ç›®",
            1: "GameFié¡¹ç›®", 
            2: "NFTé¡¹ç›®",
            3: "Layer2é¡¹ç›®",
            4: "DAOé¡¹ç›®",
            5: "åŸºç¡€è®¾æ–½",
            6: "äº¤æ˜“æ‰€",
            7: "é’±åŒ…æœåŠ¡",
            8: "æ•°æ®åˆ†æ",
            9: "å…¶ä»–"
        }
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        words = jieba.cut(text)
        word_freq = {}
        
        for word in words:
            if len(word) > 1:  # è¿‡æ»¤å•å­—ç¬¦
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘æ¬¡æ’åº
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:top_k]]
    
    def classify_content(self, content: str) -> dict:
        """åˆ†ç±»å†…å®¹"""
        # é¢„å¤„ç†æ–‡æœ¬
        words = jieba.cut(content)
        processed_text = " ".join(words)
        
        # ç‰¹å¾æå–
        features = self.vectorizer.transform([processed_text])
        
        # é¢„æµ‹åˆ†ç±»
        cluster = self.classifier.predict(features)[0]
        category = self.categories.get(cluster, "å…¶ä»–")
        
        # æå–å…³é”®è¯
        keywords = self.extract_keywords(content)
        
        return {
            "category": category,
            "keywords": keywords,
            "confidence": 0.8  # ç®€åŒ–çš„ç½®ä¿¡åº¦
        }

# åœ¨é¡¹ç›®å¤„ç†ä¸­ä½¿ç”¨
async def enhance_project_with_classification(project: TGEProject):
    """ä¸ºé¡¹ç›®æ·»åŠ åˆ†ç±»å’Œæ ‡ç­¾"""
    classifier = ContentClassifier()
    classification = classifier.classify_content(project.raw_content)
    
    # æ›´æ–°é¡¹ç›®ä¿¡æ¯
    await TGEProjectCRUD.update_ai_analysis(
        session=db,
        project_id=project.id,
        analysis_data={
            "project_category": classification["category"],
            "auto_keywords": ",".join(classification["keywords"])
        }
    )
```

### R10 - æ·»åŠ ç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†
**ç›®çš„ï¼š** ä¸ºåç»­å¤šç”¨æˆ·æ”¯æŒåšå‡†å¤‡

**å®ç°å»ºè®®ï¼š**
```python
from fastapi_users import FastAPIUsers, BaseUserManager
from fastapi_users.authentication import JWTAuthentication
from fastapi_users.db import SQLAlchemyUserDatabase

# ç”¨æˆ·æ¨¡å‹
class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False)
    hashed_password = Column(String(255), nullable=False)
    is_active = Column(Boolean, default=True)
    is_superuser = Column(Boolean, default=False)
    api_quota = Column(Integer, default=1000)  # APIè°ƒç”¨é…é¢
    created_at = Column(TIMESTAMP, server_default=func.now())

# æƒé™è£…é¥°å™¨
def require_permission(permission: str):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # æ£€æŸ¥ç”¨æˆ·æƒé™
            current_user = kwargs.get('current_user')
            if not current_user:
                raise HTTPException(401, "Authentication required")
            
            if not check_user_permission(current_user, permission):
                raise HTTPException(403, "Insufficient permissions")
            
            return await func(*args, **kwargs)
        return wrapper
    return decorator

# åœ¨è·¯ç”±ä¸­ä½¿ç”¨
@app.get("/api/v1/admin/stats")
@require_permission("admin.view_stats")
async def get_admin_stats(current_user: User = Depends(get_current_user)):
    """ç®¡ç†å‘˜ç»Ÿè®¡ä¿¡æ¯"""
    pass
```

---

## ğŸ“ˆ ä¼˜å…ˆçº§å»ºè®®

### ğŸ”¥ ç«‹å³å¤„ç†ï¼ˆ1-2å¤©ï¼‰
1. **S01-S10** - æ‰€æœ‰ä¸¥é‡å®‰å…¨é—®é¢˜
2. **I01** - æ•°æ®æ¨¡å‹ä¸ä¸€è‡´é—®é¢˜
3. **I02** - é”™è¯¯å¤„ç†æ ‡å‡†åŒ–

### âš ï¸ çŸ­æœŸå¤„ç†ï¼ˆ1å‘¨å†…ï¼‰
4. **I03-I06** - APIç‰ˆæœ¬æ§åˆ¶ã€æ€§èƒ½ä¼˜åŒ–ã€é…ç½®éªŒè¯ã€æµ‹è¯•è¦†ç›–
5. **R01-R03** - ç›‘æ§æŒ‡æ ‡ã€é…ç½®çƒ­é‡è½½ã€APIæ–‡æ¡£

### ğŸ’¡ ä¸­æœŸå¤„ç†ï¼ˆ2-4å‘¨å†…ï¼‰
6. **I07-I10** - æ—¥å¿—å®Œå–„ã€é›†æˆä¼˜åŒ–ã€å»é‡æ”¹è¿›ã€å“åº”ç»Ÿä¸€
7. **R04-R07** - æ•°æ®åº“è¿ç§»ã€ç¼“å­˜ç­–ç•¥ã€å¥åº·æ£€æŸ¥ã€ä»»åŠ¡é˜Ÿåˆ—

### ğŸ¯ é•¿æœŸè§„åˆ’ï¼ˆ1-3ä¸ªæœˆï¼‰
8. **R08-R10** - æ•°æ®å¯¼å‡ºã€å†…å®¹åˆ†ç±»ã€ç”¨æˆ·æƒé™ç³»ç»Ÿ

---

## ğŸ“ æ€»ç»“

è¯¥é¡¹ç›®åœ¨åŠŸèƒ½å®ç°ä¸Šè¾ƒä¸ºå®Œæ•´ï¼Œä½†åœ¨å®‰å…¨æ€§ã€ä»£ç è´¨é‡å’Œç”Ÿäº§ç¯å¢ƒå‡†å¤‡æ–¹é¢å­˜åœ¨è¾ƒå¤šé—®é¢˜ã€‚å»ºè®®æŒ‰ç…§ä¼˜å…ˆçº§é€æ­¥è§£å†³ï¼Œç‰¹åˆ«æ˜¯å®‰å…¨ç›¸å…³çš„ä¸¥é‡é—®é¢˜éœ€è¦ç«‹å³å¤„ç†ã€‚

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ï¼š**
1. ç«‹å³ä¿®å¤æ‰€æœ‰ä¸¥é‡å®‰å…¨æ¼æ´
2. å»ºç«‹ä»£ç è´¨é‡æ ‡å‡†å’Œæ£€æŸ¥æµç¨‹  
3. å®Œå–„æµ‹è¯•è¦†ç›–ç‡
4. æ·»åŠ ç›‘æ§å’Œæ—¥å¿—åˆ†æ
5. å‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ–¹æ¡ˆ

---

*å®¡æŸ¥æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š2025-01-14*  
*å»ºè®®å®šæœŸæ›´æ–°ï¼šæ¯æœˆä¸€æ¬¡å…¨é¢å®¡æŸ¥*