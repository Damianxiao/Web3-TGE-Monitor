#!/usr/bin/env python3
"""
æµ‹è¯•MediaCrawleråŸç”Ÿç¨‹åºèƒ½å¦æˆåŠŸçˆ¬å–çŸ¥ä¹
"""
import sys
import os
import asyncio
from pathlib import Path

# æ·»åŠ MediaCrawlerè·¯å¾„
mediacrawler_path = Path(__file__).parent / "../MediaCrawler"
sys.path.insert(0, str(mediacrawler_path.resolve()))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONPATH'] = str(mediacrawler_path.resolve())

async def test_mediacrawler_zhihu():
    """æµ‹è¯•MediaCrawlerçŸ¥ä¹çˆ¬å–"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•MediaCrawleråŸç”ŸçŸ¥ä¹çˆ¬å–...")
    
    # åˆ‡æ¢åˆ°MediaCrawlerç›®å½•
    original_dir = os.getcwd()
    mediacrawler_dir = str(Path(__file__).parent / "../MediaCrawler")
    os.chdir(mediacrawler_dir)
    print(f"ğŸ“‚ åˆ‡æ¢åˆ°MediaCrawlerç›®å½•: {mediacrawler_dir}")
    
    try:
        # å¯¼å…¥MediaCrawleræ¨¡å—
        print("ğŸ“¦ å¯¼å…¥MediaCrawleræ¨¡å—...")
        import config
        from media_platform.zhihu import ZhihuCrawler
        
        # é…ç½®MediaCrawlerå‚æ•°
        print("âš™ï¸ é…ç½®MediaCrawlerå‚æ•°...")
        config.PLATFORM = "zhihu"
        config.LOGIN_TYPE = "cookie"
        config.CRAWLER_TYPE = "search"
        config.KEYWORDS = "Web3,DeFi"
        config.SAVE_DATA_OPTION = "json"
        config.HEADLESS = True
        config.CRAWLER_MAX_NOTES_COUNT = 5  # é™åˆ¶æ•°é‡ç”¨äºæµ‹è¯•
        config.ENABLE_GET_COMMENTS = False  # ç¦ç”¨è¯„è®ºä»¥åŠ å¿«æµ‹è¯•
        
        # è®¾ç½®çŸ¥ä¹Cookie
        zhihu_cookie = os.getenv("ZHIHU_COOKIE", "")
        if not zhihu_cookie:
            # ä»æˆ‘ä»¬çš„.envæ–‡ä»¶è¯»å–
            env_path = Path(original_dir) / ".env"
            if env_path.exists():
                with open(env_path) as f:
                    for line in f:
                        if line.startswith("ZHIHU_COOKIE="):
                            zhihu_cookie = line.split("=", 1)[1].strip().strip('"')
                            break
        
        if not zhihu_cookie:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ZHIHU_COOKIEé…ç½®")
            return False
        
        config.COOKIES = zhihu_cookie
        print(f"ğŸª ä½¿ç”¨Cookie (é•¿åº¦: {len(zhihu_cookie)})")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        print("ğŸ•·ï¸ åˆ›å»ºZhihuCrawlerå®ä¾‹...")
        crawler = ZhihuCrawler()
        
        # å¼€å§‹çˆ¬å–
        print("ğŸ¯ å¼€å§‹æ‰§è¡Œçˆ¬å–...")
        await crawler.start()
        
        print("âœ… MediaCrawlerçŸ¥ä¹çˆ¬å–å®Œæˆï¼")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        data_dir = Path(mediacrawler_dir) / "data"
        if data_dir.exists():
            json_files = list(data_dir.glob("*.json"))
            if json_files:
                print(f"ğŸ“„ ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶: {json_files}")
                # è¯»å–å¹¶æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
                import json
                with open(json_files[0]) as f:
                    data = json.load(f)
                    if data:
                        print(f"ğŸ“Š è·å–åˆ° {len(data)} æ¡æ•°æ®")
                        if len(data) > 0:
                            first_item = data[0]
                            print(f"ğŸ“ ç¬¬ä¸€æ¡æ•°æ®ç±»å‹: {first_item.get('content_type', 'unknown')}")
                            print(f"ğŸ“ ç¬¬ä¸€æ¡æ•°æ®æ ‡é¢˜: {first_item.get('title', '')[:50]}...")
                        return True
                    else:
                        print("âš ï¸ æ•°æ®æ–‡ä»¶ä¸ºç©º")
                        return False
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç”Ÿæˆçš„JSONæ–‡ä»¶")
                return False
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°dataç›®å½•")
            return False
            
    except ImportError as e:
        print(f"âŒ MediaCrawleræ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ MediaCrawleræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¢å¤åŸå§‹å·¥ä½œç›®å½•
        os.chdir(original_dir)


async def test_minimal_zhihu_client():
    """æµ‹è¯•æœ€å°åŒ–çš„çŸ¥ä¹å®¢æˆ·ç«¯"""
    print("\nğŸ§ª æµ‹è¯•æœ€å°åŒ–çŸ¥ä¹å®¢æˆ·ç«¯...")
    
    # åˆ‡æ¢åˆ°MediaCrawlerç›®å½•
    original_dir = os.getcwd()
    mediacrawler_dir = str(Path(__file__).parent / "../MediaCrawler")
    os.chdir(mediacrawler_dir)
    print(f"ğŸ“‚ åˆ‡æ¢åˆ°MediaCrawlerç›®å½•: {mediacrawler_dir}")
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        sys.path.insert(0, mediacrawler_dir)
        
        from media_platform.zhihu.client import ZhiHuClient
        from media_platform.zhihu.field import SearchSort, SearchTime, SearchType
        import httpx
        
        # è·å–Cookie
        zhihu_cookie = os.getenv("ZHIHU_COOKIE", "")
        if not zhihu_cookie:
            env_path = Path(original_dir) / ".env"
            if env_path.exists():
                with open(env_path) as f:
                    for line in f:
                        if line.startswith("ZHIHU_COOKIE="):
                            zhihu_cookie = line.split("=", 1)[1].strip().strip('"')
                            break
        
        if not zhihu_cookie:
            print("âŒ æ²¡æœ‰ZHIHU_COOKIE")
            return False
        
        # è§£æCookie
        cookie_dict = {}
        for item in zhihu_cookie.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookie_dict[key] = value
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
            "cookie": zhihu_cookie,
        }
        
        client = ZhiHuClient(
            timeout=20,
            headers=headers,
            playwright_page=None,
            cookie_dict=cookie_dict
        )
        
        # æµ‹è¯•æœç´¢
        print("ğŸ” æ‰§è¡Œæœç´¢è¯·æ±‚...")
        results = await client.get_note_by_keyword(
            keyword="Web3",
            page=1,
            page_size=5,
            sort=SearchSort.DEFAULT,
            note_type=SearchType.DEFAULT,
            search_time=SearchTime.DEFAULT
        )
        
        if results:
            print(f"âœ… æœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} æ¡ç»“æœ")
            for i, result in enumerate(results[:2]):
                print(f"ğŸ“ ç»“æœ {i+1}: {result.title[:50]}...")
            return True
        else:
            print("âš ï¸ æœç´¢è¿”å›ç©ºç»“æœ")
            return False
            
    except Exception as e:
        print(f"âŒ æœ€å°åŒ–å®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¢å¤åŸå§‹å·¥ä½œç›®å½•
        os.chdir(original_dir)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª MediaCrawleråŸç”Ÿç¨‹åºæµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # æµ‹è¯•1: å®Œæ•´MediaCrawlerç¨‹åº
    success1 = await test_mediacrawler_zhihu()
    
    # æµ‹è¯•2: æœ€å°åŒ–å®¢æˆ·ç«¯
    success2 = await test_minimal_zhihu_client()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"MediaCrawlerå®Œæ•´ç¨‹åº: {'âœ… æˆåŠŸ' if success1 else 'âŒ å¤±è´¥'}")
    print(f"æœ€å°åŒ–çŸ¥ä¹å®¢æˆ·ç«¯: {'âœ… æˆåŠŸ' if success2 else 'âŒ å¤±è´¥'}")
    
    if success1 or success2:
        print("\nğŸ‰ è‡³å°‘ä¸€ç§æ–¹å¼æˆåŠŸï¼MediaCrawleråŸç”Ÿç¨‹åºå¯ä»¥å·¥ä½œ")
        print("ğŸ’¡ å»ºè®®: ç›´æ¥é›†æˆMediaCrawlerè€Œä¸æ˜¯é‡æ–°å®ç°")
    else:
        print("\nğŸ’¥ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
        print("ğŸ’¡ å¯èƒ½åŸå› :")
        print("   1. Cookieå·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°è·å–")
        print("   2. çŸ¥ä¹åŠ å¼ºäº†åçˆ¬è™«æœºåˆ¶") 
        print("   3. MediaCrawlerç‰ˆæœ¬ä¸çŸ¥ä¹APIä¸å…¼å®¹")


if __name__ == "__main__":
    asyncio.run(main())