"""
å°çº¢ä¹¦å¹³å°é€‚é…å™¨ - æ•´åˆç‰ˆ
ç›´æ¥ä½¿ç”¨é¡¹ç›®å†…éƒ¨çš„mediacrawleræ¨¡å—
"""
import json
import sys
import os
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import structlog

from ..base_platform import AbstractPlatform, PlatformError, PlatformUnavailableError
from ..models import RawContent, Platform, ContentType

logger = structlog.get_logger()


class XHSPlatform(AbstractPlatform):
    """å°çº¢ä¹¦å¹³å°å®ç° - æ•´åˆç‰ˆ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        
        # ä»é…ç½®è·å–mediacrawlerè·¯å¾„ï¼Œç¡®ä¿ä¸å…¶ä»–å¹³å°ä¸€è‡´
        self.mediacrawler_path = config.get('mediacrawler_path', '') if config else ''
        if not self.mediacrawler_path:
            # å¦‚æœé…ç½®ä¸­æ²¡æœ‰è·¯å¾„ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            import os
            from pathlib import Path
            # é¦–å…ˆå°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡
            env_path = os.getenv('MEDIACRAWLER_PATH')
            if env_path:
                self.mediacrawler_path = env_path
            else:
                # æœ€åä½¿ç”¨é»˜è®¤çš„ç›¸å¯¹è·¯å¾„
                project_root = Path(__file__).parent.parent.parent.parent
                self.mediacrawler_path = str(project_root / "external" / "MediaCrawler")
        
        # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
        import os
        from pathlib import Path
        if not os.path.isabs(self.mediacrawler_path):
            project_root = Path(__file__).parent.parent.parent.parent
            self.mediacrawler_path = str(project_root / self.mediacrawler_path)
            
        self._xhs_client = None
        
        # ç¡®ä¿mediacrawleråœ¨Pythonè·¯å¾„ä¸­
        self._ensure_mediacrawler_in_path()
        
    def _ensure_mediacrawler_in_path(self):
        """ç¡®ä¿mediacrawlerè·¯å¾„åœ¨Pythonè·¯å¾„ä¸­"""
        if self.mediacrawler_path not in sys.path:
            sys.path.insert(0, self.mediacrawler_path)
            self.logger.info("Added mediacrawler to Python path", path=self.mediacrawler_path)
        
    def get_platform_name(self) -> Platform:
        """è·å–å¹³å°åç§°"""
        return Platform.XHS
    
    async def is_available(self) -> bool:
        """æ£€æŸ¥å¹³å°æ˜¯å¦å¯ç”¨"""
        original_cwd = os.getcwd()
        try:
            # éªŒè¯mediacrawlerç›®å½•ç»“æ„
            mediacrawler_path = Path(self.mediacrawler_path)
            required_files = [
                mediacrawler_path / "media_platform" / "xhs" / "core.py",
                mediacrawler_path / "media_platform" / "xhs" / "client.py",
                mediacrawler_path / "base" / "base_crawler.py"
            ]
            
            for required_file in required_files:
                if not required_file.exists():
                    self.logger.error("Required file not found", file=str(required_file))
                    return False
            
            # åˆ‡æ¢åˆ°mediacrawlerç›®å½•ä»¥ç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
            os.chdir(self.mediacrawler_path)
            
            # å°è¯•å¯¼å…¥mediacrawlerçš„XHSæ¨¡å—
            from media_platform.xhs import client as xhs_client
            from media_platform.xhs import core as xhs_core
            
            self.logger.info("XHS platform modules imported successfully")
            return True
            
        except Exception as e:
            self.logger.error("XHS platform not available", error=str(e))
            return False
        finally:
            # æ¢å¤åŸå·¥ä½œç›®å½•
            os.chdir(original_cwd)
    
    async def _get_xhs_client(self):
        """è·å–XHSçˆ¬è™«å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._xhs_client is None:
            original_cwd = os.getcwd()
            try:
                # åˆ‡æ¢åˆ°mediacrawlerç›®å½•ä»¥ç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
                os.chdir(self.mediacrawler_path)
                
                # å¯¼å…¥MediaCrawlerçš„XHSæ ¸å¿ƒçˆ¬è™«
                from media_platform.xhs.core import XiaoHongShuCrawler
                
                # åˆ›å»ºçˆ¬è™«å®ä¾‹
                self._xhs_client = XiaoHongShuCrawler()
                
                self.logger.info("XHS crawler initialized")
                
            except Exception as e:
                self.logger.error("Failed to initialize XHS crawler", error=str(e))
                raise PlatformError("xhs", f"Failed to initialize XHS crawler: {str(e)}")
            finally:
                # æ¢å¤åŸå·¥ä½œç›®å½•
                os.chdir(original_cwd)
        
        return self._xhs_client
    
    async def crawl(
        self, 
        keywords: List[str], 
        max_count: int = 50,
        **kwargs
    ) -> List[RawContent]:
        """
        çˆ¬å–å°çº¢ä¹¦å†…å®¹ - æ–°çš„å…±äº«åº“å®ç°
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            max_count: æœ€å¤§çˆ¬å–æ•°é‡
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            çˆ¬å–åˆ°çš„å†…å®¹åˆ—è¡¨
        """
        original_cwd = os.getcwd()
        original_keywords = None
        config_file_path = None
        
        try:
            # åˆ‡æ¢åˆ°mediacrawlerç›®å½•
            os.chdir(self.mediacrawler_path)
            
            # é¦–å…ˆä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆåœ¨ä»»ä½•MediaCrawlerå¯¼å…¥ä¹‹å‰ï¼‰
            try:
                import sys
                
                # æ¸…é™¤æ‰€æœ‰MediaCrawlerç›¸å…³çš„æ¨¡å—ç¼“å­˜
                modules_to_remove = []
                for module_name in sys.modules.keys():
                    if any(keyword in module_name for keyword in ['config', 'media_platform', 'mediacrawler']):
                        modules_to_remove.append(module_name)
                        
                for module_name in modules_to_remove:
                    del sys.modules[module_name]
                
                # è¯»å–å¹¶ä¿®æ”¹é…ç½®æ–‡ä»¶
                config_file_path = os.path.join(self.mediacrawler_path, "config", "base_config.py")
                
                with open(config_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æŸ¥æ‰¾å¹¶æ›¿æ¢KEYWORDSè¡Œ
                import re
                pattern = r'KEYWORDS\s*=\s*"([^"]*)"'
                match = re.search(pattern, content)
                
                if match:
                    original_keywords = match.group(1)
                    new_keywords = ",".join(keywords)
                    new_content = re.sub(pattern, f'KEYWORDS = "{new_keywords}"', content)
                    
                    # å†™å…¥ä¸´æ—¶ä¿®æ”¹
                    with open(config_file_path, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    
                    self.logger.info("Updated MediaCrawler keywords before import", 
                                   original=original_keywords, 
                                   new=new_keywords)
                else:
                    self.logger.warning("Could not find KEYWORDS pattern in config file")
                    
            except Exception as e:
                self.logger.warning("Failed to update MediaCrawler keywords", error=str(e))
            
            # éªŒè¯å…³é”®è¯
            validated_keywords = await self.validate_keywords(keywords)
            
            self.logger.info("Starting XHS crawl with shared library",
                           keywords=validated_keywords,
                           max_count=max_count)
            
            # è·å–XHSçˆ¬è™«
            crawler = await self._get_xhs_client()
            
            # ä½¿ç”¨å®Œæ•´çš„MediaCrawleræ–¹å¼è¿›è¡Œæœç´¢
            raw_data = await self._search_with_complete_mediacrawler(validated_keywords, max_count)
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            raw_contents = []
            for item in raw_data:
                try:
                    content = await self.transform_to_raw_content(item)
                    raw_contents.append(content)
                except Exception as e:
                    self.logger.warning("Failed to transform content", 
                                      content_id=item.get('note_id', 'unknown'),
                                      error=str(e))
            
            # è¿‡æ»¤å†…å®¹
            filtered_contents = await self.filter_content(raw_contents)
            
            self.logger.info("XHS crawl completed",
                            keywords=validated_keywords,
                            raw_count=len(raw_data),
                            transformed_count=len(raw_contents),
                            filtered_count=len(filtered_contents))
            
            return filtered_contents
            
        except Exception as e:
            self.logger.error("XHS crawl failed", error=str(e))
            
            # å¦‚æœåŸå§‹å¼‚å¸¸åŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œä¿ç•™å®ƒä»¬
            if hasattr(e, 'detailed_errors'):
                platform_error = PlatformError("xhs", f"Crawl failed: {str(e)}")
                platform_error.detailed_errors = e.detailed_errors
                raise platform_error
            else:
                raise PlatformError("xhs", f"Crawl failed: {str(e)}")
        finally:
            # æ¢å¤åŸå·¥ä½œç›®å½•
            os.chdir(original_cwd)
            
            # æ¢å¤åŸå§‹å…³é”®è¯é…ç½®
            try:
                if original_keywords is not None and config_file_path and os.path.exists(config_file_path):
                    with open(config_file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ¢å¤åŸå§‹å…³é”®è¯
                    import re
                    pattern = r'KEYWORDS\s*=\s*"([^"]*)"'
                    restored_content = re.sub(pattern, f'KEYWORDS = "{original_keywords}"', content)
                    
                    with open(config_file_path, 'w', encoding='utf-8') as f:
                        f.write(restored_content)
                    
                    self.logger.info("Restored original MediaCrawler keywords", keywords=original_keywords)
            except Exception as e:
                self.logger.warning("Failed to restore original keywords", error=str(e))
    
    async def _search_with_complete_mediacrawler(self, keywords: List[str], max_count: int) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨å®Œæ•´çš„MediaCrawleræ–¹å¼è¿›è¡Œæœç´¢ï¼Œå®Œå…¨æŒ‰ç…§MediaCrawlerçš„åŸç”Ÿå®ç°
        """
        original_cwd = os.getcwd()
        try:
            # åˆ‡æ¢åˆ°mediacrawlerç›®å½•
            os.chdir(self.mediacrawler_path)
            
            # å¯¼å…¥å®Œæ•´çš„MediaCrawleræ ¸å¿ƒæ¨¡å—
            from media_platform.xhs.core import XiaoHongShuCrawler
            from playwright.async_api import async_playwright
            import config
            
            self.logger.info("Starting complete MediaCrawler search", keywords=keywords, max_count=max_count)
            
            # åˆ›å»ºå®Œæ•´çš„MediaCrawlerçˆ¬è™«å®ä¾‹
            xhs_crawler = XiaoHongShuCrawler()
            all_notes = []
            
            async with async_playwright() as playwright:
                # å¯åŠ¨æµè§ˆå™¨ï¼ŒæŒ‰ç…§MediaCrawlerçš„æ ‡å‡†æ–¹å¼
                chromium = playwright.chromium
                xhs_crawler.browser_context = await xhs_crawler.launch_browser(
                    chromium, None, xhs_crawler.user_agent, headless=config.HEADLESS
                )
                
                # æ·»åŠ åˆå§‹åŒ–è„šæœ¬
                await xhs_crawler.browser_context.add_init_script(path="libs/stealth.min.js")
                await xhs_crawler.browser_context.add_cookies([
                    {
                        "name": "webId",
                        "value": "xxx123", 
                        "domain": ".xiaohongshu.com",
                        "path": "/",
                    }
                ])
                
                # åˆ›å»ºé¡µé¢
                xhs_crawler.context_page = await xhs_crawler.browser_context.new_page()
                await xhs_crawler.context_page.goto(xhs_crawler.index_url)
                
                # åˆ›å»ºå®¢æˆ·ç«¯
                xhs_crawler.xhs_client = await xhs_crawler.create_xhs_client(None)
                
                # æ£€æŸ¥ç™»å½•çŠ¶æ€
                if not await xhs_crawler.xhs_client.pong():
                    self.logger.info("MediaCrawler: connection test failed, performing login")
                    from media_platform.xhs.login import XiaoHongShuLogin
                    
                    login_obj = XiaoHongShuLogin(
                        login_type=config.LOGIN_TYPE,
                        login_phone="",
                        browser_context=xhs_crawler.browser_context,
                        context_page=xhs_crawler.context_page,
                        cookie_str=config.COOKIES,
                    )
                    await login_obj.begin()
                    await xhs_crawler.xhs_client.update_cookies(
                        browser_context=xhs_crawler.browser_context
                    )
                else:
                    self.logger.info("MediaCrawler: connection test passed")
                
                # æ‰§è¡Œæœç´¢ï¼ŒæŒ‰ç…§MediaCrawlerçš„æœç´¢é€»è¾‘
                from media_platform.xhs.field import SearchSortType
                from media_platform.xhs.help import get_search_id
                
                xhs_limit_count = 20  # XHSæ¯é¡µå›ºå®šé™åˆ¶
                
                for keyword in keywords:
                    self.logger.info("MediaCrawler search for keyword", keyword=keyword)
                    
                    search_id = get_search_id()
                    page = 1
                    
                    try:
                        notes_res = await xhs_crawler.xhs_client.get_note_by_keyword(
                            keyword=keyword,
                            search_id=search_id,
                            page=page,
                            sort=SearchSortType.MOST_POPULAR  # ä½¿ç”¨çƒ­é—¨æ’åº
                        )
                        
                        self.logger.info("MediaCrawler search result", 
                                       keyword=keyword,
                                       has_items=bool(notes_res and notes_res.get("items")),
                                       item_count=len(notes_res.get("items", [])) if notes_res else 0)
                        
                        if not notes_res or not notes_res.get("items"):
                            self.logger.warning("No notes found for keyword", keyword=keyword)
                            continue
                        
                        # è·å–ç¬”è®°è¯¦æƒ…
                        for post_item in notes_res.get("items", []):
                            if post_item.get("model_type") in ("rec_query", "hot_query"):
                                continue
                                
                            try:
                                note_detail = await xhs_crawler.xhs_client.get_note_by_id(
                                    note_id=post_item.get("id"),
                                    xsec_source=post_item.get("xsec_source"),
                                    xsec_token=post_item.get("xsec_token")
                                )
                                
                                if note_detail:
                                    # æ·»åŠ æ¥æºå…³é”®è¯å’Œé¢å¤–ä¿¡æ¯
                                    note_detail['source_keyword'] = keyword
                                    note_detail.update({
                                        "xsec_token": post_item.get("xsec_token"), 
                                        "xsec_source": post_item.get("xsec_source")
                                    })
                                    all_notes.append(note_detail)
                                    
                                    self.logger.debug("Found note with complete MediaCrawler", 
                                                    note_id=post_item.get("id"),
                                                    keyword=keyword)
                                    
                                    # æ§åˆ¶æ€»æ•°
                                    if len(all_notes) >= max_count:
                                        break
                                        
                            except Exception as e:
                                self.logger.warning("Failed to get note detail", 
                                                  note_id=post_item.get("id"),
                                                  error=str(e))
                                continue
                        
                        self.logger.info("Found notes for keyword", 
                                       keyword=keyword, 
                                       count=len([n for n in all_notes if n.get('source_keyword') == keyword]))
                    
                    except Exception as e:
                        self.logger.error("Failed to search keyword with complete MediaCrawler", 
                                        keyword=keyword, 
                                        error=str(e))
                        continue
                    
                    # æ§åˆ¶æ€»æ•°
                    if len(all_notes) >= max_count:
                        break
                
                # å…³é—­æµè§ˆå™¨
                await xhs_crawler.close()
            
            # æˆªå–åˆ°æŒ‡å®šæ•°é‡
            result = all_notes[:max_count]
            
            self.logger.info("Complete MediaCrawler search completed", 
                           total_found=len(all_notes), 
                           returned=len(result))
            
            return result
            
        except Exception as e:
            self.logger.error("Complete MediaCrawler search failed", error=str(e))
            raise PlatformError("xhs", f"Complete MediaCrawler search failed: {str(e)}")
        finally:
            # æ¢å¤åŸå·¥ä½œç›®å½•
            os.chdir(original_cwd)
    
    async def _search_notes_with_mediacrawler_style(
        self, 
        xhs_client, 
        keywords: List[str], 
        max_count: int
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨MediaCrawleræ ‡å‡†æ–¹å¼æœç´¢ç¬”è®°
        
        Args:
            crawler: XHSçˆ¬è™«å®ä¾‹
            keywords: å…³é”®è¯åˆ—è¡¨
            max_count: æœ€å¤§æ•°é‡
            
        Returns:
            åŸå§‹æ•°æ®åˆ—è¡¨
        """
        original_cwd = os.getcwd()
        try:
            # åˆ‡æ¢åˆ°mediacrawlerç›®å½•ä»¥ç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
            os.chdir(self.mediacrawler_path)
            
            # å¯¼å…¥MediaCrawlerçš„æœç´¢ç›¸å…³æ¨¡å—
            from media_platform.xhs.field import SearchSortType
            from media_platform.xhs.help import get_search_id
            
            all_notes = []
            xhs_limit_count = 20  # XHSæ¯é¡µå›ºå®šé™åˆ¶
            
            for keyword in keywords:
                self.logger.info("Searching for keyword with MediaCrawler style", keyword=keyword)
                
                try:
                    page = 1
                    search_id = get_search_id()
                    
                    # ä½¿ç”¨MediaCrawlerçš„æ ‡å‡†æœç´¢å‚æ•°
                    notes_res = await xhs_client.get_note_by_keyword(
                        keyword=keyword,
                        search_id=search_id,
                        page=page,
                        page_size=min(xhs_limit_count, max_count),
                        sort=SearchSortType.MOST_POPULAR  # ä½¿ç”¨çƒ­é—¨æ’åºï¼Œä¸MediaCrawleré…ç½®ä¸€è‡´
                    )
                    
                    self.logger.info("MediaCrawler style search result", 
                                   keyword=keyword,
                                   has_items=bool(notes_res and notes_res.get("items")),
                                   item_count=len(notes_res.get("items", [])) if notes_res else 0)
                    
                    if not notes_res or not notes_res.get("items"):
                        self.logger.warning("No notes found for keyword", keyword=keyword)
                        continue
                    
                    # è·å–ç¬”è®°è¯¦æƒ…ï¼Œè·ŸMediaCrawlerä¸€æ ·çš„é€»è¾‘
                    for post_item in notes_res.get("items", []):
                        if post_item.get("model_type") in ("rec_query", "hot_query"):
                            continue
                            
                        try:
                            note_detail = await xhs_client.get_note_by_id(
                                note_id=post_item.get("id"),
                                xsec_source=post_item.get("xsec_source"),
                                xsec_token=post_item.get("xsec_token")
                            )
                            
                            if note_detail:
                                # æ·»åŠ æ¥æºå…³é”®è¯
                                note_detail['source_keyword'] = keyword
                                all_notes.append(note_detail)
                                
                                self.logger.debug("Found note with MediaCrawler style", 
                                                note_id=post_item.get("id"),
                                                keyword=keyword)
                                
                                # æ§åˆ¶æ€»æ•°
                                if len(all_notes) >= max_count:
                                    break
                                    
                        except Exception as e:
                            self.logger.warning("Failed to get note detail", 
                                              note_id=post_item.get("id"),
                                              error=str(e))
                            continue
                    
                    self.logger.info("Found notes for keyword", 
                                   keyword=keyword, 
                                   count=len([n for n in all_notes if n.get('source_keyword') == keyword]))
                
                except Exception as e:
                    self.logger.error("Failed to search keyword with MediaCrawler style", 
                                    keyword=keyword, 
                                    error=str(e))
                    continue
                
                # æ§åˆ¶æ€»æ•°
                if len(all_notes) >= max_count:
                    break
            
            # æˆªå–åˆ°æŒ‡å®šæ•°é‡
            result = all_notes[:max_count]
            
            self.logger.info("MediaCrawler style search completed", 
                           total_found=len(all_notes), 
                           returned=len(result))
            
            return result
            
        except Exception as e:
            self.logger.error("MediaCrawler style search failed", error=str(e))
            raise PlatformError("xhs", f"MediaCrawler style search failed: {str(e)}")
        finally:
            # æ¢å¤åŸå·¥ä½œç›®å½•
            os.chdir(original_cwd)
    
    async def _create_xhs_client_mediacrawler_style(self, crawler):
        """æŒ‰ç…§MediaCrawleræ–¹å¼åˆ›å»ºXHSå®¢æˆ·ç«¯"""
        original_cwd = os.getcwd()
        try:
            # åˆ‡æ¢åˆ°mediacrawlerç›®å½•ä»¥ç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
            os.chdir(self.mediacrawler_path)
            
            # å¯¼å…¥MediaCrawlerçš„XHSå®¢æˆ·ç«¯
            from media_platform.xhs.client import XiaoHongShuClient
            from tools import utils
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å’Œé¡µé¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not hasattr(crawler, 'browser_context') or not crawler.browser_context:
                # å¦‚æœæ²¡æœ‰æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œéœ€è¦åˆ›å»º
                from playwright.async_api import async_playwright
                import config
                
                async with async_playwright() as playwright:
                    chromium = playwright.chromium
                    crawler.browser_context = await crawler.launch_browser(
                        chromium, None, crawler.user_agent, headless=config.HEADLESS
                    )
                    crawler.context_page = await crawler.browser_context.new_page()
                    await crawler.context_page.goto("https://www.xiaohongshu.com")
            
            # è·å–cookieså¹¶åˆ›å»ºå®¢æˆ·ç«¯
            cookie_str, cookie_dict = utils.convert_cookies(
                await crawler.browser_context.cookies()
            )
            
            xhs_client = XiaoHongShuClient(
                proxies=None,  # æš‚æ—¶ä¸ä½¿ç”¨ä»£ç†
                headers={
                    "User-Agent": crawler.user_agent,
                    "Cookie": cookie_str,
                    "Origin": "https://www.xiaohongshu.com",
                    "Referer": "https://www.xiaohongshu.com",
                    "Content-Type": "application/json;charset=UTF-8",
                },
                playwright_page=crawler.context_page,
                cookie_dict=cookie_dict,
            )
            
            self.logger.info("MediaCrawler-style XHS client created successfully")
            return xhs_client
            
        except Exception as e:
            self.logger.error("Failed to create MediaCrawler-style XHS client", error=str(e))
            raise PlatformError("xhs", f"Failed to create XHS client: {str(e)}")
        finally:
            # æ¢å¤åŸå·¥ä½œç›®å½•
            os.chdir(original_cwd)
    
    async def _perform_mediacrawler_login(self, crawler):
        """æŒ‰ç…§MediaCrawleræ–¹å¼æ‰§è¡Œç™»å½•"""
        try:
            # ç¡®ä¿å¯¼å…¥æ­£ç¡®çš„MediaCrawleré…ç½®
            mediacrawler_config_path = os.path.join(self.mediacrawler_path, 'config')
            if mediacrawler_config_path not in sys.path:
                sys.path.insert(0, mediacrawler_config_path)
            import base_config as config
            
            from media_platform.xhs.login import XiaoHongShuLogin
            
            self.logger.info("Starting MediaCrawler-style login process", login_type=config.LOGIN_TYPE)
            
            # åˆ›å»ºç™»å½•å®ä¾‹
            login_instance = XiaoHongShuLogin(
                login_type=config.LOGIN_TYPE,
                login_phone="",  # input your phone number if needed
                browser_context=crawler.browser_context,
                context_page=crawler.context_page,
                cookie_str=config.COOKIES if config.LOGIN_TYPE == "cookie" else ""
            )
            
            # æ‰§è¡Œç™»å½•
            await login_instance.begin()
            
            self.logger.info("MediaCrawler-style login process completed successfully")
            
        except Exception as e:
            self.logger.error("MediaCrawler-style login process failed", error=str(e))
            raise
    
    async def _ensure_login_status(self, crawler):
        """ç¡®ä¿ç™»å½•çŠ¶æ€æœ‰æ•ˆ"""
        try:
            # ç¡®ä¿å¯¼å…¥æ­£ç¡®çš„MediaCrawleré…ç½®
            mediacrawler_config_path = os.path.join(self.mediacrawler_path, 'config')
            if mediacrawler_config_path not in sys.path:
                sys.path.insert(0, mediacrawler_config_path)
            import base_config as config
            
            self.logger.info("Checking login configuration", login_type=config.LOGIN_TYPE)
            
            if config.LOGIN_TYPE == "qrcode":
                # å¦‚æœé…ç½®ä¸ºæ‰«ç ç™»å½•ï¼Œæ€»æ˜¯æ‰§è¡Œç™»å½•æµç¨‹
                self.logger.info("QR code login configured, initiating login process")
                await self._perform_login(crawler)
            elif config.LOGIN_TYPE == "cookie":
                # Cookieç™»å½•æ¨¡å¼ï¼Œæ£€æŸ¥cookieæ˜¯å¦æœ‰æ•ˆ
                self.logger.info("Cookie login configured, validating cookie")
                cookie_valid = await self._validate_xhs_cookie()
                
                if not cookie_valid:
                    self.logger.warning("XHS Cookie validation failed, attempting refresh")
                    # å°è¯•åˆ·æ–°cookieæˆ–æä¾›æŒ‡å¯¼
                    await self._handle_invalid_cookie()
                else:
                    self.logger.info("XHS Cookie validation passed")
            
        except Exception as e:
            self.logger.warning("Login status check failed", error=str(e))
    
    async def _validate_xhs_cookie(self) -> bool:
        """éªŒè¯XHS Cookieæœ‰æ•ˆæ€§"""
        try:
            # ä»è®¾ç½®æˆ–é…ç½®è·å–cookie
            cookie_str = self.config.get('xhs_cookie', '') if hasattr(self, 'config') and self.config else ''
            
            if not cookie_str:
                # å°è¯•ä»MediaCrawleré…ç½®è·å–
                try:
                    import config
                    cookie_str = getattr(config, 'COOKIES', '')
                except (ImportError, AttributeError):
                    pass
            
            if not cookie_str:
                self.logger.warning("No XHS cookie found for validation")
                return False
            
            # åŸºæœ¬æ ¼å¼æ£€æŸ¥
            if len(cookie_str) < 50:  # Cookieåº”è¯¥æ¯”è¾ƒé•¿
                self.logger.warning("XHS cookie appears too short", length=len(cookie_str))
                return False
            
            # æ£€æŸ¥å¿…è¦çš„cookieå­—æ®µ
            required_cookie_parts = ['a1', 'webId', 'web_session']
            found_parts = []
            
            for part in required_cookie_parts:
                if part in cookie_str:
                    found_parts.append(part)
            
            if len(found_parts) < 2:
                self.logger.warning("XHS cookie missing required parts", 
                                  found=found_parts, 
                                  required=required_cookie_parts)
                return False
            
            self.logger.info("XHS cookie format validation passed", 
                           found_parts=found_parts, 
                           cookie_length=len(cookie_str))
            
            # TODO: å¯ä»¥æ·»åŠ å®é™…APIè°ƒç”¨éªŒè¯cookieæœ‰æ•ˆæ€§
            # è¿™é‡Œæš‚æ—¶è¿”å›Trueï¼Œå› ä¸ºæ ¼å¼æ£€æŸ¥é€šè¿‡
            return True
            
        except Exception as e:
            self.logger.error("XHS cookie validation failed", error=str(e))
            return False
    
    async def _handle_invalid_cookie(self):
        """å¤„ç†æ— æ•ˆçš„Cookie"""
        self.logger.error("XHS Cookieæ— æ•ˆæˆ–å·²è¿‡æœŸ")
        
        # æä¾›ç”¨æˆ·æŒ‡å¯¼
        cookie_guidance = """
        ğŸ”„ XHS Cookieå·²è¿‡æœŸï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ›´æ–°ï¼š
        
        1. æµè§ˆå™¨æ‰“å¼€ https://www.xiaohongshu.com å¹¶ç™»å½•
        2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…· -> Networkæ ‡ç­¾
        3. åˆ·æ–°é¡µé¢ï¼Œç‚¹å‡»ä»»æ„è¯·æ±‚æŸ¥çœ‹Request Headers
        4. å¤åˆ¶Cookieå®Œæ•´å€¼
        5. æ›´æ–°ç¯å¢ƒå˜é‡ XHS_COOKIE æˆ–é…ç½®æ–‡ä»¶
        6. é‡å¯åº”ç”¨
        
        æ³¨æ„ï¼šCookieé€šå¸¸åŒ…å« a1ã€webIdã€web_session ç­‰å­—æ®µ
        """
        
        self.logger.info("Cookie refresh guidance", guidance=cookie_guidance)
        
        # å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†ï¼Œæˆ–è€…å°è¯•å…¶ä»–ç™»å½•æ–¹å¼
        raise PlatformError("xhs", "XHS Cookieæ— æ•ˆï¼Œéœ€è¦æ‰‹åŠ¨æ›´æ–°ã€‚è¯·æŸ¥çœ‹æ—¥å¿—è·å–æ›´æ–°æŒ‡å¯¼ã€‚")
    
    async def _refresh_xhs_cookie(self):
        """å°è¯•åˆ·æ–°XHS Cookieï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰"""
        try:
            self.logger.info("Attempting to refresh XHS cookie")
            
            # å¯¹äºXHSï¼Œé€šå¸¸éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ›´æ–°cookie
            # è¿™é‡Œå¯ä»¥å®ç°è‡ªåŠ¨åŒ–çš„cookieåˆ·æ–°é€»è¾‘ï¼Œä½†éœ€è¦å°å¿ƒä¸è¦è¿åæœåŠ¡æ¡æ¬¾
            
            # æš‚æ—¶è¿”å›Falseï¼Œè¡¨ç¤ºéœ€è¦æ‰‹åŠ¨åˆ·æ–°
            self.logger.warning("Automatic cookie refresh not implemented for XHS")
            self.logger.info("Please manually update XHS_COOKIE environment variable")
            
            return False
            
        except Exception as e:
            self.logger.error("Failed to refresh XHS cookie", error=str(e))
            return False
    
    async def _perform_login(self, crawler):
        """æ‰§è¡Œç™»å½•æµç¨‹"""
        try:
            # ç¡®ä¿å¯¼å…¥æ­£ç¡®çš„MediaCrawleré…ç½®
            mediacrawler_config_path = os.path.join(self.mediacrawler_path, 'config')
            if mediacrawler_config_path not in sys.path:
                sys.path.insert(0, mediacrawler_config_path)
            import base_config as config
            
            from media_platform.xhs.login import XiaoHongShuLogin
            from playwright.async_api import async_playwright
            
            self.logger.info("Starting login process", login_type=config.LOGIN_TYPE)
            
            async with async_playwright() as playwright:
                # æ ¹æ®é…ç½®é€‰æ‹©å¯åŠ¨æ¨¡å¼
                if config.ENABLE_CDP_MODE:
                    browser_context = await crawler.launch_browser_with_cdp(
                        playwright, None, crawler.user_agent,
                        headless=config.CDP_HEADLESS
                    )
                else:
                    chromium = playwright.chromium
                    browser_context = await crawler.launch_browser(
                        chromium, None, crawler.user_agent, headless=config.HEADLESS
                    )
                
                # åˆ›å»ºé¡µé¢å¹¶å¯¼èˆªåˆ°å°çº¢ä¹¦
                context_page = await browser_context.new_page()
                await context_page.goto("https://www.xiaohongshu.com")
                
                # åˆ›å»ºç™»å½•å®ä¾‹
                login_instance = XiaoHongShuLogin(
                    login_type=config.LOGIN_TYPE,
                    browser_context=browser_context,
                    context_page=context_page,
                    cookie_str=config.COOKIES if config.LOGIN_TYPE == "cookie" else ""
                )
                
                # æ‰§è¡Œç™»å½•
                await login_instance.begin()
                
                self.logger.info("Login process completed successfully")
                
        except Exception as e:
            self.logger.error("Login process failed", error=str(e))
            raise
    
    async def transform_to_raw_content(self, xhs_data: Dict[str, Any]) -> RawContent:
        """
        å°†XHSæ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        
        Args:
            xhs_data: XHSåŸå§‹æ•°æ®
            
        Returns:
            æ ‡å‡†åŒ–çš„RawContent
        """
        try:
            # æå–åŸºç¡€ä¿¡æ¯
            note_id = xhs_data.get('note_id', '')
            title = xhs_data.get('title', '')
            desc = xhs_data.get('desc', '')
            content_type = xhs_data.get('type', 'text')
            
            # ç»„åˆå†…å®¹
            full_content = f"{title}\n{desc}".strip()
            
            # å¤„ç†æ—¶é—´æˆ³
            publish_time = self._parse_timestamp(xhs_data.get('time'))
            last_update_time = self._parse_timestamp(xhs_data.get('last_update_time'))
            
            # å¤„ç†åª’ä½“URL
            image_urls = []
            video_urls = []
            
            if content_type == "video" and xhs_data.get('video_url'):
                video_urls.append(xhs_data['video_url'])
            
            # å¤„ç†å›¾ç‰‡URL - MediaCrawleræ ¼å¼
            if xhs_data.get('image_list'):
                img_list = xhs_data['image_list']
                if isinstance(img_list, str):
                    image_urls.append(img_list)
                elif isinstance(img_list, list):
                    for img in img_list:
                        if isinstance(img, dict):
                            # MediaCrawlerè¿”å›çš„æ˜¯å¤æ‚å¯¹è±¡ï¼Œæå–URL
                            img_url = img.get('url') or img.get('url_default') or ''
                            if img_url:
                                image_urls.append(img_url)
                        elif isinstance(img, str):
                            image_urls.append(img)
            
            # å¤„ç†æ ‡ç­¾ - MediaCrawleræ ¼å¼
            tags = []
            if xhs_data.get('tag_list'):
                tag_list = xhs_data['tag_list']
                if isinstance(tag_list, str):
                    tags = [tag.strip() for tag in tag_list.split(',') if tag.strip()]
                elif isinstance(tag_list, list):
                    for tag in tag_list:
                        if isinstance(tag, dict):
                            # MediaCrawlerè¿”å›çš„æ˜¯å¯¹è±¡ï¼Œæå–name
                            tag_name = tag.get('name', '')
                            if tag_name:
                                tags.append(tag_name)
                        elif isinstance(tag, str):
                            tags.append(tag)
            
            # åˆ›å»ºRawContentå¯¹è±¡
            raw_content = RawContent(
                platform=Platform.XHS,
                content_id=note_id,
                content_type=ContentType.VIDEO if content_type == "video" else ContentType.TEXT,
                title=title,
                content=desc,
                raw_content=json.dumps(xhs_data, ensure_ascii=False),
                author_id=xhs_data.get('user_id', ''),
                author_name=xhs_data.get('nickname', ''),
                author_avatar=xhs_data.get('avatar', ''),
                publish_time=publish_time,
                crawl_time=datetime.utcnow(),
                last_update_time=last_update_time,
                like_count=self._parse_count(xhs_data.get('liked_count')),
                comment_count=self._parse_count(xhs_data.get('comment_count')),
                share_count=self._parse_count(xhs_data.get('share_count')),
                collect_count=self._parse_count(xhs_data.get('collected_count')),
                image_urls=image_urls,
                video_urls=video_urls,
                tags=tags,
                source_url=xhs_data.get('note_url', ''),
                ip_location=xhs_data.get('ip_location', ''),
                platform_metadata={
                    'xsec_token': xhs_data.get('xsec_token', ''),
                    'last_modify_ts': xhs_data.get('last_modify_ts'),
                    'source_keyword': xhs_data.get('source_keyword', '')
                },
                source_keywords=[xhs_data.get('source_keyword', '')] if xhs_data.get('source_keyword') else []
            )
            
            return raw_content
            
        except Exception as e:
            raise PlatformError("xhs", f"Failed to transform XHS data: {str(e)}")
    
    def _parse_timestamp(self, timestamp_value: Any) -> Optional[datetime]:
        """è§£ææ—¶é—´æˆ³"""
        if not timestamp_value:
            return None
        
        try:
            if isinstance(timestamp_value, (int, float)):
                # æ¯«ç§’æ—¶é—´æˆ³
                if timestamp_value > 10**12:
                    return datetime.fromtimestamp(timestamp_value / 1000)
                # ç§’æ—¶é—´æˆ³
                else:
                    return datetime.fromtimestamp(timestamp_value)
            return None
        except Exception:
            return None
    
    def _parse_count(self, count_value: Any) -> int:
        """è§£ææ•°é‡å­—æ®µ"""
        if not count_value:
            return 0
        
        try:
            if isinstance(count_value, int):
                return count_value
            
            if isinstance(count_value, str):
                # å¤„ç†ä¸­æ–‡æ•°å­—ï¼š1.2ä¸‡ -> 12000
                if 'ä¸‡' in count_value:
                    return int(float(count_value.replace('ä¸‡', '')) * 10000)
                elif 'åƒ' in count_value:
                    return int(float(count_value.replace('åƒ', '')) * 1000)
                elif count_value.isdigit():
                    return int(count_value)
            
            return 0
        except Exception:
            return 0